#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Result Merger - Sistema de fus√£o de resultados de classifica√ß√£o

Este m√≥dulo √© respons√°vel por:
1. Carregar dataset original de participantes
2. Carregar resultados de classifica√ß√£o de seguros
3. Fazer matching usando nomes normalizados de organiza√ß√µes
4. Adicionar coluna 'is_insurance' ao dataset original
5. Exportar resultado final em m√∫ltiplos formatos

√â como um "casamenteiro" que conecta os dados originais com os resultados da IA!
"""

import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any
import sys

# Adicionar src ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger_config import setup_logger
from core.cache_manager import CacheManager


class ResultMerger:
    """
    Sistema de fus√£o de resultados de classifica√ß√£o com dataset original
    
    Pensa nele como um "casamenteiro" que:
    - Pega a lista original de participantes
    - Pega os resultados da classifica√ß√£o IA
    - Conecta os dois usando nomes de organiza√ß√µes
    - Cria dataset final com informa√ß√£o de seguros
    """
    
    def __init__(self, cache_manager: Optional[CacheManager] = None):
        """
        Inicializa o merger de resultados
        
        Args:
            cache_manager: Gerenciador de cache (opcional)
        """
        self.logger, _ = setup_logger("result_merger", log_to_file=True)
        
        self.cache_manager = cache_manager or CacheManager()
        
        # Estat√≠sticas do merge
        self.merge_stats = {
            'total_participants': 0,
            'total_organizations': 0,
            'organizations_classified': 0,
            'organizations_not_classified': 0,
            'insurance_organizations': 0,
            'non_insurance_organizations': 0,
            'classification_rate': 0.0
        }
        
        self.logger.info("üîó Result Merger inicializado")
    
    def load_original_dataset(self, file_path: str) -> pd.DataFrame:
        """
        Carrega o dataset original de participantes
        
        Args:
            file_path: Caminho para o arquivo (CSV ou Excel)
            
        Returns:
            DataFrame com dados originais
        """
        self.logger.info(f"üìÇ Carregando dataset original: {file_path}")
        
        try:
            file_path = Path(file_path)
            
            if not file_path.exists():
                raise FileNotFoundError(f"Arquivo n√£o encontrado: {file_path}")
            
            # Detectar formato do arquivo
            if file_path.suffix.lower() == '.csv':
                df = pd.read_csv(file_path)
            elif file_path.suffix.lower() in ['.xlsx', '.xls']:
                df = pd.read_excel(file_path)
            else:
                raise ValueError(f"Formato de arquivo n√£o suportado: {file_path.suffix}")
            
            self.logger.info(f"‚úÖ Dataset carregado: {len(df)} linhas, {len(df.columns)} colunas")
            self.logger.debug(f"Colunas: {list(df.columns)}")
            
            return df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar dataset: {str(e)}")
            raise
    
    def load_classification_results(self) -> Dict[str, Dict[str, Any]]:
        """
        Carrega resultados de classifica√ß√£o do cache
        
        Returns:
            Dicion√°rio com resultados por organiza√ß√£o
        """
        self.logger.info("üì¶ Carregando resultados de classifica√ß√£o do cache")
        
        results = {}
        
        try:
            # Listar organiza√ß√µes que t√™m resultados completos
            cached_orgs = self.cache_manager.list_cached_organizations('full_results')
            
            self.logger.info(f"üìã Encontradas {len(cached_orgs)} organiza√ß√µes com resultados")
            
            for org_name in cached_orgs:
                try:
                    # Carregar resultado completo
                    result_data = self.cache_manager.load_from_cache('full_results', org_name)
                    
                    if result_data and result_data.get('success'):
                        results[org_name] = {
                            'is_insurance': result_data.get('is_insurance'),
                            'website_url': result_data.get('website_url'),
                            'content_source_type': result_data.get('content_source_type'),
                            'search_method': result_data.get('search_method'),
                            'processing_time': result_data.get('total_time_seconds', 0),
                            'success': True
                        }
                        
                        self.logger.debug(f"‚úÖ {org_name}: {'SEGURADORA' if result_data.get('is_insurance') else 'N√ÉO-SEGURADORA'}")
                    else:
                        # Resultado com falha
                        results[org_name] = {
                            'is_insurance': None,
                            'website_url': None,
                            'content_source_type': None,
                            'search_method': None,
                            'processing_time': 0,
                            'success': False,
                            'error': result_data.get('error_message', 'Unknown error') if result_data else 'No data'
                        }
                        
                        self.logger.debug(f"‚ùå {org_name}: FALHA")
                
                except Exception as e:
                    self.logger.warning(f"‚ö†Ô∏è Erro ao carregar resultado para {org_name}: {str(e)}")
                    continue
            
            self.logger.info(f"‚úÖ Carregados resultados para {len(results)} organiza√ß√µes")
            
            # Estat√≠sticas dos resultados
            successful = len([r for r in results.values() if r['success']])
            insurance = len([r for r in results.values() if r.get('is_insurance') is True])
            non_insurance = len([r for r in results.values() if r.get('is_insurance') is False])
            
            self.logger.info(f"üìä Resultados: {successful} sucessos, {insurance} seguradoras, {non_insurance} n√£o-seguradoras")
            
            return results
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao carregar resultados de classifica√ß√£o: {str(e)}")
            return {}
    
    def create_people_dataset(self, processed_df: pd.DataFrame, 
                             organizations_df: pd.DataFrame,
                             org_column: str = 'Home organization') -> pd.DataFrame:
        """
        Cria dataset final de pessoas com APENAS coluna is_insurance adicionada
        
        Args:
            processed_df: DataFrame processado e normalizado (merged_data_normalized.csv)
            organizations_df: DataFrame de organiza√ß√µes com classifica√ß√µes (results/organizations.csv)
            org_column: Nome da coluna com organiza√ß√µes
            
        Returns:
            DataFrame final para results/people.csv
        """
        self.logger.info("üë• Criando dataset final de pessoas")
        
        try:
            # Criar c√≥pia do DataFrame processado
            people_df = processed_df.copy()
            
            # Simplificar colunas de organiza√ß√£o se necess√°rio
            if 'Home organization_normalized' in people_df.columns and 'Home organization' in people_df.columns:
                # Substituir coluna original pela normalizada
                people_df['Home organization'] = people_df['Home organization_normalized']
                people_df.drop('Home organization_normalized', axis=1, inplace=True)
                self.logger.info("‚úÖ Coluna 'Home organization' atualizada com valores normalizados")
            
            # Verificar se coluna de organiza√ß√£o existe
            if org_column not in people_df.columns:
                available_cols = [col for col in people_df.columns if 'organization' in col.lower()]
                if available_cols:
                    org_column = available_cols[0]
                    self.logger.warning(f"‚ö†Ô∏è Coluna '{org_column}' n√£o encontrada, usando '{org_column}'")
                else:
                    raise ValueError(f"Coluna de organiza√ß√£o n√£o encontrada. Colunas dispon√≠veis: {list(people_df.columns)}")
            
            # Inicializar APENAS coluna is_insurance (conforme processo definido)
            people_df['is_insurance'] = None
            
            # Criar dicion√°rio de lookup das organiza√ß√µes
            org_lookup = {}
            for _, org_row in organizations_df.iterrows():
                org_name = org_row.get('organization_name', '')
                if org_name:
                    org_lookup[org_name] = org_row.get('is_insurance', None)
            
            # Estat√≠sticas
            total_rows = len(people_df)
            unique_orgs = people_df[org_column].nunique()
            matched_count = 0
            insurance_people = 0
            non_insurance_people = 0
            
            self.logger.info(f"üìä Dataset de pessoas: {total_rows} participantes, {unique_orgs} organiza√ß√µes √∫nicas")
            self.logger.info(f"üìã Organiza√ß√µes classificadas dispon√≠veis: {len(org_lookup)}")
            
            # Fazer matching linha por linha - APENAS is_insurance
            for idx, row in people_df.iterrows():
                org_name = row[org_column]
                
                if pd.isna(org_name) or not org_name:
                    continue
                
                # Limpar nome da organiza√ß√£o
                org_name_clean = str(org_name).strip()
                
                # Procurar classifica√ß√£o na tabela de organiza√ß√µes
                if org_name_clean in org_lookup:
                    is_insurance = org_lookup[org_name_clean]
                    people_df.at[idx, 'is_insurance'] = is_insurance
                    matched_count += 1
                    
                    if is_insurance is True:
                        insurance_people += 1
                    elif is_insurance is False:
                        non_insurance_people += 1
            
            # Calcular estat√≠sticas finais
            classification_rate = (matched_count / total_rows * 100) if total_rows > 0 else 0
            
            self.merge_stats = {
                'total_participants': total_rows,
                'participants_with_classification': matched_count,
                'participants_without_classification': total_rows - matched_count,
                'insurance_participants': insurance_people,
                'non_insurance_participants': non_insurance_people,
                'classification_rate': classification_rate
            }
            
            self.logger.info("‚úÖ Dataset de pessoas criado com sucesso!")
            self.logger.info(f"üìä Estat√≠sticas:")
            self.logger.info(f"   ‚Ä¢ Total de participantes: {total_rows}")
            self.logger.info(f"   ‚Ä¢ Participantes classificados: {matched_count} ({classification_rate:.1f}%)")
            self.logger.info(f"   ‚Ä¢ Participantes de seguradoras: {insurance_people}")
            self.logger.info(f"   ‚Ä¢ Participantes de n√£o-seguradoras: {non_insurance_people}")
            
            return people_df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no merge: {str(e)}")
            raise
    
    def create_multi_file_organizations_csv(self, data: pd.DataFrame, 
                                          organizations_df: pd.DataFrame) -> pd.DataFrame:
        """
        Cria organizations.csv V2.0 com contagens de participantes por arquivo
        
        Args:
            data: DataFrame principal com coluna 'File'
            organizations_df: DataFrame com resultados de classifica√ß√£o
            
        Returns:
            DataFrame de organiza√ß√µes com colunas participants_{filename}
        """
        self.logger.info("üìä Criando organizations.csv V2.0 com contagens multi-arquivo...")
        
        try:
            # Determinar coluna de organiza√ß√£o
            org_column = 'Organization' if 'Organization' in data.columns else 'Home organization'
            
            # Obter arquivos √∫nicos
            if 'File' not in data.columns:
                self.logger.warning("‚ö†Ô∏è Coluna 'File' n√£o encontrada, usando contagem √∫nica")
                unique_files = ['total']
            else:
                unique_files = sorted(data['File'].unique())
            
            self.logger.info(f"üìÅ Arquivos encontrados: {unique_files}")
            
            # Contar participantes por organiza√ß√£o e arquivo
            if 'File' in data.columns:
                counts_by_file = data.groupby([org_column, 'File']).size().unstack(fill_value=0)
            else:
                counts_by_file = data.groupby(org_column).size().to_frame('total')
            
            # Calcular total
            counts_by_file['participants_total'] = counts_by_file.sum(axis=1)
            
            # Renomear colunas para formato participants_{filename}
            column_mapping = {}
            for col in counts_by_file.columns:
                if col != 'participants_total':
                    column_mapping[col] = f'participants_{col}'
            
            counts_by_file = counts_by_file.rename(columns=column_mapping)
            
            # Criar DataFrame final combinando com dados de classifica√ß√£o
            final_orgs_df = organizations_df.copy()
            
            # Fazer merge com contagens
            final_orgs_df = final_orgs_df.merge(
                counts_by_file, 
                left_on='organization_name', 
                right_index=True, 
                how='left'
            )
            
            # Preencher valores ausentes com 0
            participant_cols = [col for col in final_orgs_df.columns if col.startswith('participants_')]
            for col in participant_cols:
                final_orgs_df[col] = final_orgs_df[col].fillna(0).astype(int)
            
            # Reordenar colunas
            base_cols = ['organization_name']
            participant_cols = sorted([col for col in final_orgs_df.columns if col.startswith('participants_')])
            other_cols = [col for col in final_orgs_df.columns if col not in base_cols + participant_cols]
            
            final_cols = base_cols + participant_cols + other_cols
            final_orgs_df = final_orgs_df[final_cols]
            
            self.logger.success(f"‚ú® Organizations.csv V2.0 criado: {len(final_orgs_df)} organiza√ß√µes")
            self.logger.info(f"üìä Colunas de participantes: {participant_cols}")
            
            return final_orgs_df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao criar organizations.csv V2.0: {str(e)}")
            raise
    
    def create_simplified_people_csv(self, data: pd.DataFrame) -> pd.DataFrame:
        """
        Cria people.csv V2.0 com coluna √∫nica de organiza√ß√£o normalizada
        
        Args:
            data: DataFrame principal com dados processados
            
        Returns:
            DataFrame de pessoas com estrutura simplificada
        """
        self.logger.info("üë• Criando people.csv V2.0 simplificado...")
        
        try:
            people_df = data.copy()
            
            # Determinar coluna de organiza√ß√£o
            org_column = 'Organization' if 'Organization' in people_df.columns else 'Home organization'
            
            # Renomear coluna de organiza√ß√£o para nome padr√£o
            if org_column != 'Home organization':
                people_df = people_df.rename(columns={org_column: 'Home organization'})
            
            # Remover colunas duplicadas de organiza√ß√£o se existirem
            cols_to_remove = []
            for col in people_df.columns:
                if 'organization' in col.lower() and col != 'Home organization':
                    cols_to_remove.append(col)
            
            if cols_to_remove:
                people_df = people_df.drop(columns=cols_to_remove)
                self.logger.info(f"üóëÔ∏è Removidas colunas duplicadas: {cols_to_remove}")
            
            # Garantir que coluna File existe
            if 'File' not in people_df.columns:
                people_df['File'] = 'unknown'
                self.logger.warning("‚ö†Ô∏è Coluna 'File' n√£o encontrada, adicionada com valor 'unknown'")
            
            # Reordenar colunas: File, Type, Nominated by, Home organization, Name, is_insurance, outras
            base_cols = ['File', 'Type', 'Nominated by', 'Home organization', 'Name']
            if 'is_insurance' in people_df.columns:
                base_cols.append('is_insurance')
            
            other_cols = [col for col in people_df.columns if col not in base_cols]
            final_cols = [col for col in base_cols if col in people_df.columns] + other_cols
            
            people_df = people_df[final_cols]
            
            self.logger.success(f"‚ú® People.csv V2.0 criado: {len(people_df)} pessoas")
            self.logger.info(f"üìä Colunas: {list(people_df.columns)}")
            
            return people_df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro ao criar people.csv V2.0: {str(e)}")
            raise
    
    def export_results(self, merged_df: pd.DataFrame, 
                      output_dir: str = "data/results",
                      base_filename: str = None) -> Dict[str, str]:
        """
        Exporta resultados em m√∫ltiplos formatos
        
        Args:
            merged_df: DataFrame com resultados merged
            output_dir: Diret√≥rio de sa√≠da
            base_filename: Nome base dos arquivos (None = auto-gerar)
            
        Returns:
            Dicion√°rio com caminhos dos arquivos gerados
        """
        self.logger.info("üíæ Exportando resultados finais")
        
        try:
            # Criar diret√≥rio de sa√≠da
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            # Gerar nome base se n√£o fornecido
            if not base_filename:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                base_filename = f"cop29_insurance_classification_{timestamp}"
            
            exported_files = {}
            
            # Exportar CSV
            csv_path = output_path / f"{base_filename}.csv"
            merged_df.to_csv(csv_path, index=False, encoding='utf-8')
            exported_files['csv'] = str(csv_path)
            self.logger.info(f"‚úÖ CSV exportado: {csv_path}")
            
            # Exportar Excel
            excel_path = output_path / f"{base_filename}.xlsx"
            with pd.ExcelWriter(excel_path, engine='openpyxl') as writer:
                # Aba principal com todos os dados
                merged_df.to_excel(writer, sheet_name='All_Participants', index=False)
                
                # Aba apenas com seguradoras
                insurance_df = merged_df[merged_df['is_insurance'] == True]
                if not insurance_df.empty:
                    insurance_df.to_excel(writer, sheet_name='Insurance_Companies', index=False)
                
                # Aba com estat√≠sticas
                stats_df = pd.DataFrame([
                    ['Total Participants', self.merge_stats['total_participants']],
                    ['Unique Organizations', self.merge_stats['total_organizations']],
                    ['Organizations Classified', self.merge_stats['organizations_classified']],
                    ['Classification Rate (%)', f"{self.merge_stats['classification_rate']:.1f}%"],
                    ['Insurance Companies', self.merge_stats['insurance_organizations']],
                    ['Non-Insurance Companies', self.merge_stats['non_insurance_organizations']],
                    ['Export Date', datetime.now().strftime("%Y-%m-%d %H:%M:%S")]
                ], columns=['Metric', 'Value'])
                
                stats_df.to_excel(writer, sheet_name='Statistics', index=False)
            
            exported_files['excel'] = str(excel_path)
            self.logger.info(f"‚úÖ Excel exportado: {excel_path}")
            
            # Exportar resumo em texto
            summary_path = output_path / f"{base_filename}_summary.txt"
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write("COP29 Insurance Classification Results\n")
                f.write("=" * 40 + "\n\n")
                f.write(f"Export Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
                
                f.write("STATISTICS:\n")
                f.write("-" * 20 + "\n")
                for key, value in self.merge_stats.items():
                    if key == 'classification_rate':
                        f.write(f"{key.replace('_', ' ').title()}: {value:.1f}%\n")
                    else:
                        f.write(f"{key.replace('_', ' ').title()}: {value}\n")
                
                f.write(f"\nFILES GENERATED:\n")
                f.write("-" * 20 + "\n")
                for file_type, file_path in exported_files.items():
                    f.write(f"{file_type.upper()}: {file_path}\n")
            
            exported_files['summary'] = str(summary_path)
            self.logger.info(f"‚úÖ Resumo exportado: {summary_path}")
            
            self.logger.info(f"üéâ Exporta√ß√£o conclu√≠da! {len(exported_files)} arquivos gerados")
            
            return exported_files
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na exporta√ß√£o: {str(e)}")
            raise
    
    def get_merge_statistics(self) -> Dict[str, Any]:
        """
        Retorna estat√≠sticas do merge
        
        Returns:
            Dicion√°rio com estat√≠sticas
        """
        return self.merge_stats.copy()
    
    def validate_merge_results(self, merged_df: pd.DataFrame) -> Dict[str, Any]:
        """
        Valida os resultados do merge
        
        Args:
            merged_df: DataFrame com resultados merged
            
        Returns:
            Dicion√°rio com valida√ß√£o
        """
        self.logger.info("üîç Validando resultados do merge")
        
        validation = {
            'total_rows': len(merged_df),
            'rows_with_classification': len(merged_df[merged_df['insurance_classification_success'] == True]),
            'rows_without_classification': len(merged_df[merged_df['insurance_classification_success'] == False]),
            'insurance_companies': len(merged_df[merged_df['is_insurance'] == True]),
            'non_insurance_companies': len(merged_df[merged_df['is_insurance'] == False]),
            'missing_data': {
                'organization_name': merged_df['Home organization'].isna().sum(),
                'classification_result': merged_df['is_insurance'].isna().sum()
            },

        }
        

        
        self.logger.info(f"üìä Valida√ß√£o conclu√≠da:")
        self.logger.info(f"   ‚Ä¢ Linhas com classifica√ß√£o: {validation['rows_with_classification']}")
        
        return validation


def main():
    """Fun√ß√£o para testar o result merger"""
    merger = ResultMerger()
    
    print("üß™ Testando Result Merger")
    print("=" * 40)
    
    # Testar carregamento de resultados de classifica√ß√£o
    print("üì¶ Carregando resultados de classifica√ß√£o...")
    results = merger.load_classification_results()
    print(f"Resultados carregados: {len(results)}")
    
    if results:
        print("\nüìã Primeiros resultados:")
        for i, (org, result) in enumerate(list(results.items())[:3]):
            status = "SEGURADORA" if result.get('is_insurance') else "N√ÉO-SEGURADORA"
            print(f"  {i+1}. {org}: {status}")
    
    print("\n‚úÖ Teste conclu√≠do!")


if __name__ == "__main__":
    main()