#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Organization Normalizer - Normaliza nomes de organiza√ß√µes usando fuzzy matching

Este m√≥dulo √© respons√°vel por:
1. Extrair organiza√ß√µes √∫nicas do dataset
2. Encontrar organiza√ß√µes similares usando fuzzy matching
3. Criar mapeamento para o nome mais frequente
4. Atualizar o dataset principal com nomes normalizados
"""

import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import sys
from collections import Counter
from rapidfuzz import fuzz, process

# Adicionar src ao path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger_config import setup_logger
from utils.config_manager import config_manager


class OrganizationNormalizer:
    """
    Normalizador de nomes de organiza√ß√µes usando fuzzy matching
    """
    
    def __init__(self):
        self.logger, _ = setup_logger("org_normalizer", log_to_file=True)
        self.fuzzy_config = config_manager.get_fuzzy_config()
        
        self.logger.info("üîß Inicializando Organization Normalizer")
        self.logger.debug(f"Threshold de similaridade: {self.fuzzy_config['threshold']}%")
        self.logger.debug(f"M√°ximo de sugest√µes: {self.fuzzy_config['max_suggestions']}")
    
    def extract_unique_organizations(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Extrai organiza√ß√µes √∫nicas do dataset com contagem de ocorr√™ncias
        
        Args:
            df: DataFrame principal com dados processados
            
        Returns:
            DataFrame com organiza√ß√µes √∫nicas e suas contagens
        """
        self.logger.info("üìä Extraindo organiza√ß√µes √∫nicas...")
        
        # Contar ocorr√™ncias de cada organiza√ß√£o
        org_counts = df['Home organization'].value_counts()
        
        # Criar DataFrame com organiza√ß√µes √∫nicas
        unique_orgs_df = pd.DataFrame({
            'original_name': org_counts.index,
            'occurrence_count': org_counts.values,
            'normalized_name': org_counts.index,  # Inicialmente igual ao original
            'is_normalized': False  # Flag para indicar se foi normalizado
        }).reset_index(drop=True)
        
        self.logger.success(f"‚ú® Encontradas {len(unique_orgs_df)} organiza√ß√µes √∫nicas")
        self.logger.info(f"üìà Total de ocorr√™ncias: {unique_orgs_df['occurrence_count'].sum()}")
        
        # Mostrar estat√≠sticas
        self.logger.info("üìä Estat√≠sticas de frequ√™ncia:")
        self.logger.info(f"   Organiza√ß√µes com 1 ocorr√™ncia: {(unique_orgs_df['occurrence_count'] == 1).sum()}")
        self.logger.info(f"   Organiza√ß√µes com 2-5 ocorr√™ncias: {((unique_orgs_df['occurrence_count'] >= 2) & (unique_orgs_df['occurrence_count'] <= 5)).sum()}")
        self.logger.info(f"   Organiza√ß√µes com 6+ ocorr√™ncias: {(unique_orgs_df['occurrence_count'] >= 6).sum()}")
        
        return unique_orgs_df
    
    def _clean_organization_name(self, name: str) -> str:
        """
        Limpa nome da organiza√ß√£o para melhor matching
        
        Args:
            name: Nome original da organiza√ß√£o
            
        Returns:
            Nome limpo para compara√ß√£o
        """
        if pd.isna(name):
            return ""
        
        # Converter para string e limpar
        cleaned = str(name).strip()
        
        # Remover caracteres especiais comuns
        cleaned = cleaned.replace(',', '').replace('.', '').replace('&', 'and')
        
        # Normalizar espa√ßos
        cleaned = ' '.join(cleaned.split())
        
        return cleaned
    
    def _validate_similarity(self, org1: str, org2: str) -> bool:
        """
        Valida se duas organiza√ß√µes s√£o realmente similares (n√£o apenas por fuzzy score)
        
        Args:
            org1: Primeira organiza√ß√£o
            org2: Segunda organiza√ß√£o
            
        Returns:
            True se s√£o realmente similares
        """
        clean1 = self._clean_organization_name(org1).lower()
        clean2 = self._clean_organization_name(org2).lower()
        
        # Palavras-chave que n√£o devem ser confundidas
        conflicting_keywords = [
            ('african', 'asian'), ('africa', 'asia'),
            ('american', 'european'), ('america', 'europe'),
            ('north', 'south'), ('east', 'west'),
            ('development', 'investment'), ('bank', 'fund'),
            ('international', 'national'), ('global', 'local')
        ]
        
        # Verificar se h√° conflitos de palavras-chave
        for word1, word2 in conflicting_keywords:
            if (word1 in clean1 and word2 in clean2) or (word2 in clean1 and word1 in clean2):
                return False
        
        # Verificar se compartilham pelo menos 50% das palavras principais
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        # Remover palavras muito comuns que n√£o s√£o distintivas
        common_words = {'the', 'of', 'and', 'for', 'in', 'on', 'at', 'to', 'a', 'an'}
        words1 = words1 - common_words
        words2 = words2 - common_words
        
        if not words1 or not words2:
            return False
        
        # Calcular sobreposi√ß√£o de palavras
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        word_overlap = len(intersection) / len(union) if union else 0
        
        # Exigir pelo menos 30% de sobreposi√ß√£o de palavras
        return word_overlap >= 0.3

    def find_similar_organizations(self, org_list: List[str]) -> Dict[str, str]:
        """
        Encontra organiza√ß√µes similares usando fuzzy matching melhorado
        
        Args:
            org_list: Lista de nomes de organiza√ß√µes
            
        Returns:
            Dicion√°rio mapeando nome original -> nome normalizado
        """
        self.logger.info("üîç Iniciando busca por organiza√ß√µes similares...")
        
        threshold = self.fuzzy_config['threshold']
        mapping = {}
        processed = set()
        groups_found = 0
        
        for i, org1 in enumerate(org_list):
            if org1 in processed:
                continue
            
            # Limpar nome para compara√ß√£o
            clean_org1 = self._clean_organization_name(org1)
            if not clean_org1:
                continue
            
            # Encontrar organiza√ß√µes similares
            similar_orgs = [(org1, org_list.index(org1))]  # (nome, √≠ndice_frequ√™ncia)
            
            for j, org2 in enumerate(org_list[i+1:], i+1):
                if org2 in processed:
                    continue
                
                clean_org2 = self._clean_organization_name(org2)
                if not clean_org2:
                    continue
                
                # Calcular similaridade
                similarity = fuzz.ratio(clean_org1.lower(), clean_org2.lower())
                
                # Aplicar valida√ß√µes adicionais
                if similarity >= threshold and self._validate_similarity(org1, org2):
                    similar_orgs.append((org2, j))
                    processed.add(org2)
            
            # Se encontrou organiza√ß√µes similares, criar mapeamento
            if len(similar_orgs) > 1:
                groups_found += 1
                
                # Escolher o nome mais frequente (menor √≠ndice = mais frequente)
                similar_orgs.sort(key=lambda x: x[1])  # Ordenar por frequ√™ncia
                normalized_name = similar_orgs[0][0]
                
                for org_name, _ in similar_orgs:
                    mapping[org_name] = normalized_name
                
                self.logger.debug(f"Grupo {groups_found}: {len(similar_orgs)} organiza√ß√µes similares -> '{normalized_name}'")
                for org_name, _ in similar_orgs[1:]:  # N√£o mostrar o primeiro (√© o normalizado)
                    self.logger.debug(f"   '{org_name}' -> '{normalized_name}'")
            
            processed.add(org1)
        
        self.logger.success(f"‚ú® Encontrados {groups_found} grupos de organiza√ß√µes similares")
        self.logger.info(f"üìä Total de mapeamentos criados: {len(mapping)}")
        
        return mapping
    
    def normalize_organization_names(self, unique_orgs_df: pd.DataFrame) -> pd.DataFrame:
        """
        Normaliza nomes de organiza√ß√µes usando fuzzy matching
        
        Args:
            unique_orgs_df: DataFrame com organiza√ß√µes √∫nicas
            
        Returns:
            DataFrame atualizado com nomes normalizados
        """
        self.logger.info("üîÑ Iniciando normaliza√ß√£o de nomes...")
        
        # Ordenar por frequ√™ncia (mais frequente primeiro)
        sorted_orgs = unique_orgs_df.sort_values('occurrence_count', ascending=False)
        org_list = sorted_orgs['original_name'].tolist()
        
        # Encontrar organiza√ß√µes similares
        mapping = self.find_similar_organizations(org_list)
        
        # Aplicar mapeamento
        normalized_count = 0
        for idx, row in unique_orgs_df.iterrows():
            original_name = row['original_name']
            
            if original_name in mapping:
                unique_orgs_df.at[idx, 'normalized_name'] = mapping[original_name]
                unique_orgs_df.at[idx, 'is_normalized'] = True
                normalized_count += 1
        
        self.logger.success(f"‚ú® Normalizadas {normalized_count} organiza√ß√µes")
        
        # Calcular estat√≠sticas finais
        final_unique_count = unique_orgs_df['normalized_name'].nunique()
        original_unique_count = len(unique_orgs_df)
        reduction = original_unique_count - final_unique_count
        reduction_pct = (reduction / original_unique_count) * 100
        
        self.logger.info(f"üìä Resultado da normaliza√ß√£o:")
        self.logger.info(f"   Organiza√ß√µes originais: {original_unique_count}")
        self.logger.info(f"   Organiza√ß√µes ap√≥s normaliza√ß√£o: {final_unique_count}")
        self.logger.info(f"   Redu√ß√£o: {reduction} organiza√ß√µes ({reduction_pct:.1f}%)")
        
        return unique_orgs_df
    
    def update_main_dataset(self, main_df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:
        """
        Atualiza o dataset principal com nomes normalizados
        
        Args:
            main_df: DataFrame principal
            mapping: Dicion√°rio de mapeamento original -> normalizado
            
        Returns:
            DataFrame atualizado
        """
        self.logger.info("üîÑ Atualizando dataset principal com nomes normalizados...")
        
        # Criar nova coluna com nomes normalizados
        main_df['Home organization_normalized'] = main_df['Home organization'].map(mapping).fillna(main_df['Home organization'])
        
        # Contar quantas linhas foram atualizadas
        updated_count = (main_df['Home organization'] != main_df['Home organization_normalized']).sum()
        
        self.logger.success(f"‚ú® Atualizadas {updated_count} linhas no dataset principal")
        
        return main_df
    
    def process_normalization(self, input_file: str = "data/processed/merged_data.csv") -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        M√©todo principal que executa todo o pipeline de normaliza√ß√£o
        
        Args:
            input_file: Caminho para o arquivo de dados processados
            
        Returns:
            Tuple com (dataset_principal_atualizado, dataframe_organizacoes_unicas)
        """
        self.logger.info("üöÄ Iniciando processo completo de normaliza√ß√£o")
        
        try:
            # 1. Carregar dados processados
            input_path = Path(input_file)
            if not input_path.exists():
                raise FileNotFoundError(f"Arquivo n√£o encontrado: {input_path}")
            
            self.logger.info(f"üìÇ Carregando dados de: {input_path}")
            main_df = pd.read_csv(input_path)
            
            # 2. Extrair organiza√ß√µes √∫nicas
            unique_orgs_df = self.extract_unique_organizations(main_df)
            
            # 3. Normalizar nomes
            normalized_orgs_df = self.normalize_organization_names(unique_orgs_df)
            
            # 4. Criar mapeamento para o dataset principal
            mapping = dict(zip(normalized_orgs_df['original_name'], normalized_orgs_df['normalized_name']))
            
            # 5. Atualizar dataset principal
            updated_main_df = self.update_main_dataset(main_df, mapping)
            
            # 6. Salvar resultados
            # Salvar dataset principal atualizado
            main_output_path = Path("data/processed/merged_data_normalized.csv")
            updated_main_df.to_csv(main_output_path, index=False, encoding='utf-8')
            self.logger.info(f"üíæ Dataset principal salvo em: {main_output_path}")
            
            # Salvar mapeamento de organiza√ß√µes
            orgs_output_path = Path("data/processed/organizations_mapping.csv")
            normalized_orgs_df.to_csv(orgs_output_path, index=False, encoding='utf-8')
            self.logger.info(f"üíæ Mapeamento de organiza√ß√µes salvo em: {orgs_output_path}")
            
            self.logger.success("‚ú® Processo de normaliza√ß√£o conclu√≠do com sucesso!")
            
            return updated_main_df, normalized_orgs_df
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro no processo de normaliza√ß√£o: {str(e)}")
            raise


def main():
    """Fun√ß√£o para testar o normalizador"""
    normalizer = OrganizationNormalizer()
    
    try:
        main_df, orgs_df = normalizer.process_normalization()
        
        print(f"\nüìä Resumo da normaliza√ß√£o:")
        print(f"Dataset principal: {main_df.shape}")
        print(f"Organiza√ß√µes √∫nicas: {len(orgs_df)}")
        print(f"Organiza√ß√µes normalizadas: {orgs_df['is_normalized'].sum()}")
        
        print(f"\nPrimeiros 5 mapeamentos:")
        normalized_orgs = orgs_df[orgs_df['is_normalized']].head()
        for _, row in normalized_orgs.iterrows():
            print(f"  '{row['original_name']}' -> '{row['normalized_name']}'")
        
    except Exception as e:
        print(f"Erro: {e}")


if __name__ == "__main__":
    main()