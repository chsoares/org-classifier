#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Organization Normalizer - Normaliza nomes de organizaÃ§Ãµes usando fuzzy matching

Este mÃ³dulo Ã© responsÃ¡vel por:
1. Extrair organizaÃ§Ãµes Ãºnicas do dataset
2. Encontrar organizaÃ§Ãµes similares usando fuzzy matching
3. Criar mapeamento para o nome mais frequente
4. Atualizar o dataset principal com nomes normalizados
"""

import pandas as pd
from pathlib import Path
from typing import Dict, List, Tuple
import sys
from collections import Counter
from rapidfuzz import fuzz, process

# Adicionar src ao path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger_config import setup_logger
from utils.config_manager import config_manager


class OrganizationNormalizer:
    """
    Normalizador de nomes de organizaÃ§Ãµes usando fuzzy matching
    """
    
    def __init__(self):
        self.logger, _ = setup_logger("org_normalizer", log_to_file=True)
        self.fuzzy_config = config_manager.get_fuzzy_config()
        
        self.logger.info("ðŸ”§ Inicializando Organization Normalizer")
        self.logger.debug(f"Threshold de similaridade: {self.fuzzy_config['threshold']}%")
        self.logger.debug(f"MÃ¡ximo de sugestÃµes: {self.fuzzy_config['max_suggestions']}")
    
    def extract_unique_organizations(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        Extrai organizaÃ§Ãµes Ãºnicas do dataset com contagem de ocorrÃªncias
        
        Args:
            df: DataFrame principal com dados processados
            
        Returns:
            DataFrame com organizaÃ§Ãµes Ãºnicas e suas contagens
        """
        self.logger.info("ðŸ“Š Extraindo organizaÃ§Ãµes Ãºnicas...")
        
        # Contar ocorrÃªncias de cada organizaÃ§Ã£o
        org_counts = df['Home organization'].value_counts()
        
        # Criar DataFrame com organizaÃ§Ãµes Ãºnicas
        unique_orgs_df = pd.DataFrame({
            'original_name': org_counts.index,
            'occurrence_count': org_counts.values,
            'normalized_name': org_counts.index,  # Inicialmente igual ao original
            'is_normalized': False  # Flag para indicar se foi normalizado
        }).reset_index(drop=True)
        
        self.logger.success(f"âœ¨ Encontradas {len(unique_orgs_df)} organizaÃ§Ãµes Ãºnicas")
        self.logger.info(f"ðŸ“ˆ Total de ocorrÃªncias: {unique_orgs_df['occurrence_count'].sum()}")
        
        # Mostrar estatÃ­sticas
        self.logger.info("ðŸ“Š EstatÃ­sticas de frequÃªncia:")
        self.logger.info(f"   OrganizaÃ§Ãµes com 1 ocorrÃªncia: {(unique_orgs_df['occurrence_count'] == 1).sum()}")
        self.logger.info(f"   OrganizaÃ§Ãµes com 2-5 ocorrÃªncias: {((unique_orgs_df['occurrence_count'] >= 2) & (unique_orgs_df['occurrence_count'] <= 5)).sum()}")
        self.logger.info(f"   OrganizaÃ§Ãµes com 6+ ocorrÃªncias: {(unique_orgs_df['occurrence_count'] >= 6).sum()}")
        
        return unique_orgs_df
    
    def _clean_organization_name(self, name: str) -> str:
        """
        Limpa nome da organizaÃ§Ã£o para melhor matching
        
        Args:
            name: Nome original da organizaÃ§Ã£o
            
        Returns:
            Nome limpo para comparaÃ§Ã£o
        """
        if pd.isna(name):
            return ""
        
        # Converter para string e limpar
        cleaned = str(name).strip()
        
        # Remover caracteres especiais comuns
        cleaned = cleaned.replace(',', '').replace('.', '').replace('&', 'and')
        
        # Normalizar espaÃ§os
        cleaned = ' '.join(cleaned.split())
        
        return cleaned
    
    def _validate_similarity(self, org1: str, org2: str) -> bool:
        """
        Valida se duas organizaÃ§Ãµes sÃ£o realmente similares (nÃ£o apenas por fuzzy score)
        
        Args:
            org1: Primeira organizaÃ§Ã£o
            org2: Segunda organizaÃ§Ã£o
            
        Returns:
            True se sÃ£o realmente similares
        """
        clean1 = self._clean_organization_name(org1).lower()
        clean2 = self._clean_organization_name(org2).lower()
        
        # Palavras-chave que nÃ£o devem ser confundidas
        conflicting_keywords = [
            ('african', 'asian'), ('africa', 'asia'),
            ('american', 'european'), ('america', 'europe'),
            ('north', 'south'), ('east', 'west'),
            ('development', 'investment'), ('bank', 'fund'),
            ('international', 'national'), ('global', 'local')
        ]
        
        # Verificar se hÃ¡ conflitos de palavras-chave
        for word1, word2 in conflicting_keywords:
            if (word1 in clean1 and word2 in clean2) or (word2 in clean1 and word1 in clean2):
                return False
        
        # Verificar se compartilham pelo menos 50% das palavras principais
        words1 = set(clean1.split())
        words2 = set(clean2.split())
        
        # Remover palavras muito comuns que nÃ£o sÃ£o distintivas
        common_words = {'the', 'of', 'and', 'for', 'in', 'on', 'at', 'to', 'a', 'an'}
        words1 = words1 - common_words
        words2 = words2 - common_words
        
        if not words1 or not words2:
            return False
        
        # Calcular sobreposiÃ§Ã£o de palavras
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        word_overlap = len(intersection) / len(union) if union else 0
        
        # Exigir pelo menos 30% de sobreposiÃ§Ã£o de palavras
        return word_overlap >= 0.3

    def find_similar_organizations(self, org_list: List[str]) -> Dict[str, str]:
        """
        Encontra organizaÃ§Ãµes similares usando fuzzy matching melhorado
        
        Args:
            org_list: Lista de nomes de organizaÃ§Ãµes
            
        Returns:
            DicionÃ¡rio mapeando nome original -> nome normalizado
        """
        self.logger.info("ðŸ” Iniciando busca por organizaÃ§Ãµes similares...")
        
        threshold = self.fuzzy_config['threshold']
        mapping = {}
        processed = set()
        groups_found = 0
        
        for i, org1 in enumerate(org_list):
            if org1 in processed:
                continue
            
            # Limpar nome para comparaÃ§Ã£o
            clean_org1 = self._clean_organization_name(org1)
            if not clean_org1:
                continue
            
            # Encontrar organizaÃ§Ãµes similares
            similar_orgs = [(org1, org_list.index(org1))]  # (nome, Ã­ndice_frequÃªncia)
            
            for j, org2 in enumerate(org_list[i+1:], i+1):
                if org2 in processed:
                    continue
                
                clean_org2 = self._clean_organization_name(org2)
                if not clean_org2:
                    continue
                
                # Calcular similaridade
                similarity = fuzz.ratio(clean_org1.lower(), clean_org2.lower())
                
                # Aplicar validaÃ§Ãµes adicionais
                if similarity >= threshold and self._validate_similarity(org1, org2):
                    similar_orgs.append((org2, j))
                    processed.add(org2)
            
            # Se encontrou organizaÃ§Ãµes similares, criar mapeamento
            if len(similar_orgs) > 1:
                groups_found += 1
                
                # Escolher o nome mais frequente (menor Ã­ndice = mais frequente)
                similar_orgs.sort(key=lambda x: x[1])  # Ordenar por frequÃªncia
                normalized_name = similar_orgs[0][0]
                
                for org_name, _ in similar_orgs:
                    mapping[org_name] = normalized_name
                
                self.logger.debug(f"Grupo {groups_found}: {len(similar_orgs)} organizaÃ§Ãµes similares -> '{normalized_name}'")
                for org_name, _ in similar_orgs[1:]:  # NÃ£o mostrar o primeiro (Ã© o normalizado)
                    self.logger.debug(f"   '{org_name}' -> '{normalized_name}'")
            
            processed.add(org1)
        
        self.logger.success(f"âœ¨ Encontrados {groups_found} grupos de organizaÃ§Ãµes similares")
        self.logger.info(f"ðŸ“Š Total de mapeamentos criados: {len(mapping)}")
        
        return mapping
    
    def normalize_organization_names(self, unique_orgs_df: pd.DataFrame) -> pd.DataFrame:
        """
        Normaliza nomes de organizaÃ§Ãµes usando fuzzy matching
        
        Args:
            unique_orgs_df: DataFrame com organizaÃ§Ãµes Ãºnicas
            
        Returns:
            DataFrame atualizado com nomes normalizados
        """
        self.logger.info("ðŸ”„ Iniciando normalizaÃ§Ã£o de nomes...")
        
        # Ordenar por frequÃªncia (mais frequente primeiro)
        sorted_orgs = unique_orgs_df.sort_values('occurrence_count', ascending=False)
        org_list = sorted_orgs['original_name'].tolist()
        
        # Encontrar organizaÃ§Ãµes similares
        mapping = self.find_similar_organizations(org_list)
        
        # Aplicar mapeamento
        normalized_count = 0
        for idx, row in unique_orgs_df.iterrows():
            original_name = row['original_name']
            
            if original_name in mapping:
                unique_orgs_df.at[idx, 'normalized_name'] = mapping[original_name]
                unique_orgs_df.at[idx, 'is_normalized'] = True
                normalized_count += 1
        
        self.logger.success(f"âœ¨ Normalizadas {normalized_count} organizaÃ§Ãµes")
        
        # Calcular estatÃ­sticas finais
        final_unique_count = unique_orgs_df['normalized_name'].nunique()
        original_unique_count = len(unique_orgs_df)
        reduction = original_unique_count - final_unique_count
        reduction_pct = (reduction / original_unique_count) * 100
        
        self.logger.info(f"ðŸ“Š Resultado da normalizaÃ§Ã£o:")
        self.logger.info(f"   OrganizaÃ§Ãµes originais: {original_unique_count}")
        self.logger.info(f"   OrganizaÃ§Ãµes apÃ³s normalizaÃ§Ã£o: {final_unique_count}")
        self.logger.info(f"   ReduÃ§Ã£o: {reduction} organizaÃ§Ãµes ({reduction_pct:.1f}%)")
        
        return unique_orgs_df
    
    def update_main_dataset(self, main_df: pd.DataFrame, mapping: Dict[str, str]) -> pd.DataFrame:
        """
        Atualiza o dataset principal com nomes normalizados
        
        Args:
            main_df: DataFrame principal
            mapping: DicionÃ¡rio de mapeamento original -> normalizado
            
        Returns:
            DataFrame atualizado
        """
        self.logger.info("ðŸ”„ Atualizando dataset principal com nomes normalizados...")
        
        # Criar nova coluna com nomes normalizados
        main_df['Home organization_normalized'] = main_df['Home organization'].map(mapping).fillna(main_df['Home organization'])
        
        # Contar quantas linhas foram atualizadas
        updated_count = (main_df['Home organization'] != main_df['Home organization_normalized']).sum()
        
        self.logger.success(f"âœ¨ Atualizadas {updated_count} linhas no dataset principal")
        
        return main_df
    
    def process_normalization(self, input_file: str = "data/processed/merged_data.csv") -> Tuple[pd.DataFrame, pd.DataFrame]:
        """
        MÃ©todo principal que executa todo o pipeline de normalizaÃ§Ã£o
        
        Args:
            input_file: Caminho para o arquivo de dados processados
            
        Returns:
            Tuple com (dataset_principal_atualizado, dataframe_organizacoes_unicas)
        """
        self.logger.info("ðŸš€ Iniciando processo completo de normalizaÃ§Ã£o")
        
        try:
            # 1. Carregar dados processados
            input_path = Path(input_file)
            if not input_path.exists():
                raise FileNotFoundError(f"Arquivo nÃ£o encontrado: {input_path}")
            
            self.logger.info(f"ðŸ“‚ Carregando dados de: {input_path}")
            main_df = pd.read_csv(input_path)
            
            # 2. Extrair organizaÃ§Ãµes Ãºnicas
            unique_orgs_df = self.extract_unique_organizations(main_df)
            
            # 3. Normalizar nomes
            normalized_orgs_df = self.normalize_organization_names(unique_orgs_df)
            
            # 4. Criar mapeamento para o dataset principal
            mapping = dict(zip(normalized_orgs_df['original_name'], normalized_orgs_df['normalized_name']))
            
            # 5. Atualizar dataset principal
            updated_main_df = self.update_main_dataset(main_df, mapping)
            
            # 6. Salvar resultados
            # Salvar dataset principal atualizado
            main_output_path = Path("data/processed/merged_data_normalized.csv")
            updated_main_df.to_csv(main_output_path, index=False, encoding='utf-8')
            self.logger.info(f"ðŸ’¾ Dataset principal salvo em: {main_output_path}")
            
            # Salvar mapeamento de organizaÃ§Ãµes
            orgs_output_path = Path("data/processed/organizations_mapping.csv")
            normalized_orgs_df.to_csv(orgs_output_path, index=False, encoding='utf-8')
            self.logger.info(f"ðŸ’¾ Mapeamento de organizaÃ§Ãµes salvo em: {orgs_output_path}")
            
            self.logger.success("âœ¨ Processo de normalizaÃ§Ã£o concluÃ­do com sucesso!")
            
            return updated_main_df, normalized_orgs_df
            
        except Exception as e:
            self.logger.error(f"âŒ Erro no processo de normalizaÃ§Ã£o: {str(e)}")
            raise


def main():
    """FunÃ§Ã£o para testar o normalizador"""
    normalizer = OrganizationNormalizer()
    
    try:
        main_df, orgs_df = normalizer.process_normalization()
        
        print(f"\nðŸ“Š Resumo da normalizaÃ§Ã£o:")
        print(f"Dataset principal: {main_df.shape}")
        print(f"OrganizaÃ§Ãµes Ãºnicas: {len(orgs_df)}")
        print(f"OrganizaÃ§Ãµes normalizadas: {orgs_df['is_normalized'].sum()}")
        
        print(f"\nPrimeiros 5 mapeamentos:")
        normalized_orgs = orgs_df[orgs_df['is_normalized']].head()
        for _, row in normalized_orgs.iterrows():
            print(f"  '{row['original_name']}' -> '{row['normalized_name']}'")
        
    except Exception as e:
        print(f"Erro: {e}")


if __name__ == "__main__":
    main()