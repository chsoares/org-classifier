#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Organization Web Extractor - Sistema de extra√ß√£o de conte√∫do de organiza√ß√µes

Este m√≥dulo √© respons√°vel por:
1. Extrair conte√∫do relevante de websites de organiza√ß√µes
2. Priorizar Wikipedia como fonte prim√°ria
3. Extrair se√ß√µes "About" de websites pr√≥prios como fallback
4. Limpar e normalizar texto extra√≠do
5. Limitar conte√∫do a 2000 caracteres mantendo relev√¢ncia
"""

import requests
import urllib3
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re
import time
from typing import Optional, Dict, List
import sys
from pathlib import Path

# Desabilitar avisos de SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Adicionar src ao path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger_config import setup_logger
from utils.config_manager import config_manager


class OrganizationWebExtractor:
    """
    Sistema de extra√ß√£o de conte√∫do web para organiza√ß√µes
    Adaptado para priorizar Wikipedia e extrair informa√ß√µes relevantes
    """
    
    def __init__(self):
        self.logger, _ = setup_logger("org_web_extractor", log_to_file=True)
        self.scraping_config = config_manager.get_scraping_config()
        
        self.logger.info("üåê Inicializando Organization Web Extractor")
        
        # Configura√ß√µes
        self.timeout = self.scraping_config['timeout']
        self.max_retries = self.scraping_config['max_retries']
        self.retry_delay = self.scraping_config['retry_delay']
        
        # Headers para requisi√ß√µes
        self.headers = {
            "User-Agent": self.scraping_config['user_agent'],
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # Palavras-chave para identificar se√ß√µes relevantes
        self.about_keywords = [
            # Ingl√™s
            "about", "about us", "company", "who we are", "our company", 
            "overview", "corporate", "organization", "mission", "vision",
            "history", "background", "profile", "description",
            # Portugu√™s
            "sobre", "quem somos", "empresa", "institucional", "nossa empresa",
            "miss√£o", "vis√£o", "hist√≥ria", "perfil", "descri√ß√£o",
            # Franc√™s
            "√† propos", "qui sommes-nous", "entreprise", "soci√©t√©", "aper√ßu",
            # Alem√£o
            "√ºber uns", "unternehmen", "wer wir sind", "firma", "√ºberblick",
            # Espanhol
            "sobre nosotros", "empresa", "qui√©nes somos", "compa√±√≠a"
        ]
        
        # Seletores CSS para conte√∫do relevante
        self.content_selectors = [
            "main", "article", ".content", ".main-content", 
            ".about", ".company", ".overview", ".description",
            "#about", "#company", "#overview", "#main"
        ]
        
        self.logger.debug(f"Configura√ß√µes: timeout={self.timeout}s, keywords={len(self.about_keywords)}")
    
    def extract_organization_content(self, url: str, org_name: str) -> Optional[Dict[str, str]]:
        """
        Extrai conte√∫do relevante de uma organiza√ß√£o
        
        Args:
            url: URL para extrair conte√∫do
            org_name: Nome da organiza√ß√£o (para valida√ß√£o)
            
        Returns:
            Dict com informa√ß√µes extra√≠das ou None
        """
        self.logger.info(f"üåê Extraindo conte√∫do para: {org_name}")
        self.logger.debug(f"URL: {url}")
        
        try:
            # Determinar tipo de fonte
            source_type = self._determine_source_type(url)
            
            # Fazer requisi√ß√£o HTTP
            response = self._make_request(url)
            if not response:
                self.logger.error(f"‚ùå Falha na requisi√ß√£o para {url}")
                return None
            
            # Parse do HTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extrair conte√∫do baseado no tipo de fonte
            if source_type == "wikipedia":
                content_data = self._extract_wikipedia_content(soup, org_name)
            else:
                content_data = self._extract_website_content(soup, org_name, url)
            
            if not content_data:
                self.logger.warning(f"‚ö†Ô∏è Nenhum conte√∫do relevante extra√≠do de {url}")
                return None
            
            # Adicionar metadados
            content_data.update({
                'source_url': url,
                'source_type': source_type,
                'extraction_timestamp': time.time()
            })
            
            self.logger.success(f"‚ú® Conte√∫do extra√≠do com sucesso: {len(content_data.get('content', ''))} caracteres")
            return content_data
            
        except Exception as e:
            self.logger.error(f"‚ùå Erro na extra√ß√£o de {url}: {str(e)}")
            return None
    
    def _determine_source_type(self, url: str) -> str:
        """
        Determina o tipo de fonte baseado na URL
        
        Args:
            url: URL para analisar
            
        Returns:
            Tipo da fonte: 'wikipedia', 'website'
        """
        if 'wikipedia.org' in url.lower():
            return 'wikipedia'
        else:
            return 'website'
    
    def _extract_wikipedia_content(self, soup: BeautifulSoup, org_name: str) -> Optional[Dict[str, str]]:
        """
        Extrai conte√∫do espec√≠fico da Wikipedia
        
        Args:
            soup: BeautifulSoup object da p√°gina
            org_name: Nome da organiza√ß√£o
            
        Returns:
            Dict com conte√∫do extra√≠do
        """
        self.logger.debug(f"üìö Extraindo conte√∫do da Wikipedia para: {org_name}")
        
        try:
            content_parts = []
            
            # 1. Extrair t√≠tulo da p√°gina
            title_elem = soup.find('h1', class_='firstHeading')
            if title_elem:
                title = title_elem.get_text().strip()
                content_parts.append(f"Title: {title}")
            
            # 2. Extrair primeiro par√°grafo (resumo)
            first_paragraph = soup.find('div', class_='mw-parser-output')
            if first_paragraph:
                # Pegar os primeiros par√°grafos antes da primeira se√ß√£o
                paragraphs = first_paragraph.find_all('p', recursive=False)
                for p in paragraphs[:3]:  # Primeiros 3 par√°grafos
                    text = self._clean_text(p.get_text())
                    if text and len(text) > 50:  # Apenas par√°grafos substanciais
                        content_parts.append(text)
            
            # 3. Extrair infobox (caixa de informa√ß√µes)
            infobox = soup.find('table', class_='infobox')
            if infobox:
                infobox_data = self._extract_infobox_data(infobox)
                if infobox_data:
                    content_parts.append(f"Key Information: {infobox_data}")
            
            # 4. Extrair se√ß√µes relevantes (History, Operations, etc.)
            relevant_sections = self._extract_wikipedia_sections(soup)
            content_parts.extend(relevant_sections)
            
            if not content_parts:
                return None
            
            # Juntar conte√∫do e limitar tamanho
            full_content = " ".join(content_parts)
            limited_content = self._limit_content_length(full_content, 2000)
            
            return {
                'content': limited_content,
                'title': title if 'title' in locals() else org_name,
                'content_type': 'wikipedia_summary'
            }
            
        except Exception as e:
            self.logger.error(f"Erro na extra√ß√£o Wikipedia: {str(e)}")
            return None
    
    def _extract_website_content(self, soup: BeautifulSoup, org_name: str, url: str) -> Optional[Dict[str, str]]:
        """
        Extrai conte√∫do de website pr√≥prio da organiza√ß√£o
        
        Args:
            soup: BeautifulSoup object da p√°gina
            org_name: Nome da organiza√ß√£o
            url: URL da p√°gina
            
        Returns:
            Dict com conte√∫do extra√≠do
        """
        self.logger.debug(f"üåê Extraindo conte√∫do do website para: {org_name}")
        
        try:
            content_parts = []
            
            # 1. Extrair t√≠tulo da p√°gina
            title = self._extract_page_title(soup)
            if title:
                content_parts.append(f"Title: {title}")
            
            # 2. Extrair conte√∫do principal usando seletores
            main_content = self._extract_main_content(soup)
            if main_content:
                content_parts.append(main_content)
            
            # 3. Buscar e extrair se√ß√µes "About"
            about_content = self._extract_about_sections(soup, url)
            if about_content:
                content_parts.extend(about_content)
            
            # 4. Extrair meta description
            meta_desc = self._extract_meta_description(soup)
            if meta_desc:
                content_parts.append(f"Description: {meta_desc}")
            
            if not content_parts:
                return None
            
            # Juntar conte√∫do e limitar tamanho
            full_content = " ".join(content_parts)
            limited_content = self._limit_content_length(full_content, 2000)
            
            return {
                'content': limited_content,
                'title': title or org_name,
                'content_type': 'website_content'
            }
            
        except Exception as e:
            self.logger.error(f"Erro na extra√ß√£o website: {str(e)}")
            return None
    
    def _extract_infobox_data(self, infobox: BeautifulSoup) -> str:
        """
        Extrai dados relevantes do infobox da Wikipedia
        
        Args:
            infobox: Elemento infobox
            
        Returns:
            String com dados formatados
        """
        try:
            data_parts = []
            
            # Procurar por linhas de dados
            rows = infobox.find_all('tr')
            for row in rows:
                header = row.find('th')
                data = row.find('td')
                
                if header and data:
                    header_text = self._clean_text(header.get_text())
                    data_text = self._clean_text(data.get_text())
                    
                    # Filtrar informa√ß√µes relevantes
                    relevant_fields = [
                        'type', 'industry', 'founded', 'headquarters', 'founder',
                        'products', 'services', 'revenue', 'employees', 'website'
                    ]
                    
                    if any(field in header_text.lower() for field in relevant_fields):
                        if len(data_text) < 200:  # Evitar dados muito longos
                            data_parts.append(f"{header_text}: {data_text}")
            
            return "; ".join(data_parts[:5])  # M√°ximo 5 campos
            
        except Exception as e:
            self.logger.debug(f"Erro na extra√ß√£o infobox: {str(e)}")
            return ""
    
    def _extract_wikipedia_sections(self, soup: BeautifulSoup) -> List[str]:
        """
        Extrai se√ß√µes relevantes da Wikipedia
        
        Args:
            soup: BeautifulSoup object
            
        Returns:
            Lista de conte√∫dos de se√ß√µes
        """
        sections = []
        
        try:
            # Procurar por headings de se√ß√µes relevantes
            relevant_headings = ['history', 'operations', 'business', 'overview', 'activities']
            
            for heading in soup.find_all(['h2', 'h3']):
                heading_text = heading.get_text().lower()
                
                if any(keyword in heading_text for keyword in relevant_headings):
                    # Extrair conte√∫do da se√ß√£o
                    section_content = []
                    
                    # Pegar par√°grafos seguintes at√© pr√≥ximo heading
                    for sibling in heading.find_next_siblings():
                        if sibling.name in ['h2', 'h3']:
                            break
                        if sibling.name == 'p':
                            text = self._clean_text(sibling.get_text())
                            if text and len(text) > 30:
                                section_content.append(text)
                    
                    if section_content:
                        section_text = " ".join(section_content[:2])  # M√°ximo 2 par√°grafos por se√ß√£o
                        if len(section_text) < 500:  # Limitar tamanho da se√ß√£o
                            sections.append(section_text)
            
        except Exception as e:
            self.logger.debug(f"Erro na extra√ß√£o de se√ß√µes: {str(e)}")
        
        return sections[:3]  # M√°ximo 3 se√ß√µes
    
    def _extract_page_title(self, soup: BeautifulSoup) -> Optional[str]:
        """
        Extrai t√≠tulo da p√°gina
        
        Args:
            soup: BeautifulSoup object
            
        Returns:
            T√≠tulo da p√°gina ou None
        """
        # Tentar diferentes seletores para t√≠tulo
        title_selectors = ['title', 'h1', '.page-title', '.main-title']
        
        for selector in title_selectors:
            element = soup.select_one(selector)
            if element:
                title = self._clean_text(element.get_text())
                if title and len(title) < 200:
                    return title
        
        return None
    
    def _extract_main_content(self, soup: BeautifulSoup) -> Optional[str]:
        """
        Extrai conte√∫do principal da p√°gina
        
        Args:
            soup: BeautifulSoup object
            
        Returns:
            Conte√∫do principal ou None
        """
        content_parts = []
        
        # Tentar seletores de conte√∫do principal
        for selector in self.content_selectors:
            elements = soup.select(selector)
            for element in elements:
                text = self._clean_text(element.get_text())
                if text and len(text) > 100:  # Apenas conte√∫do substancial
                    content_parts.append(text)
                    break  # Pegar apenas o primeiro conte√∫do relevante
            
            if content_parts:
                break
        
        # Se n√£o encontrou conte√∫do espec√≠fico, pegar par√°grafos principais
        if not content_parts:
            paragraphs = soup.find_all('p')
            for p in paragraphs[:5]:  # Primeiros 5 par√°grafos
                text = self._clean_text(p.get_text())
                if text and len(text) > 50:
                    content_parts.append(text)
        
        return " ".join(content_parts[:3]) if content_parts else None  # M√°ximo 3 partes
    
    def _extract_about_sections(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """
        Busca e extrai se√ß√µes "About" da p√°gina
        
        Args:
            soup: BeautifulSoup object
            base_url: URL base para resolver links relativos
            
        Returns:
            Lista de conte√∫dos de se√ß√µes "About"
        """
        about_sections = []
        
        try:
            # 1. Procurar se√ß√µes "About" na p√°gina atual
            for keyword in self.about_keywords:
                # Procurar por IDs
                element = soup.find(id=lambda x: x and keyword.lower() in x.lower())
                if element:
                    text = self._clean_text(element.get_text())
                    if text and len(text) > 50:
                        about_sections.append(text)
                        continue
                
                # Procurar por classes
                element = soup.find(class_=lambda x: x and keyword.lower() in ' '.join(x).lower())
                if element:
                    text = self._clean_text(element.get_text())
                    if text and len(text) > 50:
                        about_sections.append(text)
                        continue
            
            # 2. Procurar links para p√°ginas "About" (limitado a 1 p√°gina)
            if not about_sections:
                about_link = self._find_about_link(soup, base_url)
                if about_link:
                    about_content = self._extract_about_page_content(about_link)
                    if about_content:
                        about_sections.append(about_content)
        
        except Exception as e:
            self.logger.debug(f"Erro na extra√ß√£o de se√ß√µes About: {str(e)}")
        
        return about_sections[:2]  # M√°ximo 2 se√ß√µes About
    
    def _find_about_link(self, soup: BeautifulSoup, base_url: str) -> Optional[str]:
        """
        Encontra link para p√°gina "About"
        
        Args:
            soup: BeautifulSoup object
            base_url: URL base
            
        Returns:
            URL da p√°gina About ou None
        """
        try:
            for link in soup.find_all('a', href=True):
                href = link.get('href', '').lower()
                link_text = link.get_text().lower()
                
                # Verificar se √© link About
                is_about_link = any(
                    keyword in link_text or keyword.replace(' ', '-') in href
                    for keyword in ['about', 'about us', 'company', 'who we are']
                )
                
                if is_about_link and href != '#':
                    # Resolver URL relativa
                    if href.startswith('/'):
                        parsed_base = urlparse(base_url)
                        about_url = f"{parsed_base.scheme}://{parsed_base.netloc}{href}"
                    elif href.startswith('http'):
                        about_url = href
                    else:
                        continue
                    
                    return about_url
        
        except Exception as e:
            self.logger.debug(f"Erro na busca de link About: {str(e)}")
        
        return None
    
    def _extract_about_page_content(self, about_url: str) -> Optional[str]:
        """
        Extrai conte√∫do de p√°gina About espec√≠fica
        
        Args:
            about_url: URL da p√°gina About
            
        Returns:
            Conte√∫do da p√°gina About ou None
        """
        try:
            response = self._make_request(about_url)
            if not response:
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Extrair conte√∫do principal da p√°gina About
            main_content = self._extract_main_content(soup)
            if main_content and len(main_content) > 100:
                return main_content[:800]  # Limitar conte√∫do About
            
        except Exception as e:
            self.logger.debug(f"Erro na extra√ß√£o p√°gina About: {str(e)}")
        
        return None
    
    def _extract_meta_description(self, soup: BeautifulSoup) -> Optional[str]:
        """
        Extrai meta description da p√°gina
        
        Args:
            soup: BeautifulSoup object
            
        Returns:
            Meta description ou None
        """
        try:
            meta_desc = soup.find('meta', attrs={'name': 'description'})
            if meta_desc and meta_desc.get('content'):
                desc = self._clean_text(meta_desc.get('content'))
                if desc and len(desc) > 20:
                    return desc
        except Exception:
            pass
        
        return None
    
    def _make_request(self, url: str) -> Optional[requests.Response]:
        """
        Faz requisi√ß√£o HTTP com retry logic
        
        Args:
            url: URL para requisi√ß√£o
            
        Returns:
            Response object ou None
        """
        for attempt in range(self.max_retries + 1):
            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    timeout=self.timeout,
                    verify=False,
                    allow_redirects=True
                )
                
                if response.status_code == 200:
                    return response
                elif response.status_code == 429:  # Rate limited
                    if attempt < self.max_retries:
                        wait_time = self.retry_delay * (2 ** attempt)
                        self.logger.debug(f"Rate limited, aguardando {wait_time}s...")
                        time.sleep(wait_time)
                        continue
                else:
                    self.logger.debug(f"Status code {response.status_code} para {url}")
                    return None
                    
            except requests.exceptions.Timeout:
                if attempt < self.max_retries:
                    self.logger.debug(f"Timeout na tentativa {attempt + 1}, tentando novamente...")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    self.logger.debug(f"Timeout final para {url}")
                    return None
                    
            except Exception as e:
                if attempt < self.max_retries:
                    self.logger.debug(f"Erro na tentativa {attempt + 1}: {str(e)}")
                    time.sleep(self.retry_delay)
                    continue
                else:
                    self.logger.debug(f"Erro final para {url}: {str(e)}")
                    return None
        
        return None
    
    def _clean_text(self, text: str) -> str:
        """
        Limpa e normaliza texto extra√≠do
        
        Args:
            text: Texto para limpar
            
        Returns:
            Texto limpo
        """
        if not text:
            return ""
        
        # Remover quebras de linha e espa√ßos extras
        text = " ".join(text.split())
        
        # Remover caracteres especiais mantendo pontua√ß√£o b√°sica
        text = re.sub(r'[^\w\s\-.,;:?!()\[\]{}"\']', ' ', text)
        
        # Remover m√∫ltiplos espa√ßos
        text = re.sub(r'\s+', ' ', text).strip()
        
        return text
    
    def _limit_content_length(self, content: str, max_length: int = 2000) -> str:
        """
        Limita o tamanho do conte√∫do mantendo as partes mais relevantes
        
        Args:
            content: Conte√∫do para limitar
            max_length: Tamanho m√°ximo em caracteres
            
        Returns:
            Conte√∫do limitado
        """
        if len(content) <= max_length:
            return content
        
        # Dividir em senten√ßas
        sentences = content.split('.')
        
        # Priorizar senten√ßas com palavras-chave relevantes
        relevant_sentences = []
        other_sentences = []
        
        keywords = ['company', 'organization', 'business', 'industry', 'founded', 'headquarters']
        
        for sentence in sentences:
            sentence = sentence.strip()
            if sentence:
                if any(keyword in sentence.lower() for keyword in keywords):
                    relevant_sentences.append(sentence)
                else:
                    other_sentences.append(sentence)
        
        # Construir conte√∫do limitado
        result = []
        current_length = 0
        
        # Adicionar senten√ßas relevantes primeiro
        for sentence in relevant_sentences:
            if current_length + len(sentence) + 1 <= max_length:
                result.append(sentence)
                current_length += len(sentence) + 1
            else:
                break
        
        # Adicionar outras senten√ßas se houver espa√ßo
        for sentence in other_sentences:
            if current_length + len(sentence) + 1 <= max_length:
                result.append(sentence)
                current_length += len(sentence) + 1
            else:
                break
        
        return '. '.join(result) + '.' if result else content[:max_length]
    
    def validate_content_relevance(self, content: str, org_name: str) -> bool:
        """
        Valida se o conte√∫do extra√≠do √© relevante para a organiza√ß√£o
        
        Args:
            content: Conte√∫do extra√≠do
            org_name: Nome da organiza√ß√£o
            
        Returns:
            True se o conte√∫do √© relevante
        """
        if not content or len(content) < 50:
            return False
        
        content_lower = content.lower()
        org_words = org_name.lower().split()
        
        # Verificar se pelo menos uma palavra da organiza√ß√£o aparece no conte√∫do
        org_match = any(word in content_lower for word in org_words if len(word) > 3)
        
        # Verificar se cont√©m palavras-chave organizacionais
        org_keywords = [
            'company', 'organization', 'corporation', 'business', 'firm',
            'enterprise', 'group', 'association', 'foundation', 'institute'
        ]
        keyword_match = any(keyword in content_lower for keyword in org_keywords)
        
        return org_match or keyword_match


def main():
    """Fun√ß√£o para testar o extractor"""
    extractor = OrganizationWebExtractor()
    
    # Testar com URLs de exemplo
    test_cases = [
        ("https://en.wikipedia.org/wiki/Microsoft", "Microsoft Corporation"),
        ("https://www.microsoft.com", "Microsoft Corporation"),
    ]
    
    print(f"\nüß™ Testando Organization Web Extractor:")
    
    for url, org_name in test_cases:
        print(f"\nüåê Testando: {org_name}")
        print(f"URL: {url}")
        
        result = extractor.extract_organization_content(url, org_name)
        
        if result:
            print(f"‚úÖ Sucesso!")
            print(f"  Tipo: {result.get('content_type')}")
            print(f"  T√≠tulo: {result.get('title')}")
            print(f"  Tamanho: {len(result.get('content', ''))} caracteres")
            print(f"  Conte√∫do (primeiros 200 chars): {result.get('content', '')[:200]}...")
        else:
            print(f"‚ùå Falha na extra√ß√£o")


if __name__ == "__main__":
    main()