#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Web Searcher - Sistema de busca de websites de organiza√ß√µes

Este m√≥dulo implementa busca inteligente com:
1. Wikipedia primeiro com valida√ß√£o de relev√¢ncia
2. Bing como fallback se Wikipedia n√£o for relevante
3. Valida√ß√£o de URLs encontradas
"""

import requests
import urllib3
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re
import time
from typing import Optional, Tuple
import sys
from pathlib import Path

# Desabilitar avisos de SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Adicionar src ao path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger_config import setup_logger
from utils.config_manager import config_manager


class WebSearcher:
    """
    Sistema de busca de websites de organiza√ß√µes com Wikipedia + Bing
    """
    
    def __init__(self):
        self.logger, _ = setup_logger("web_searcher", log_to_file=True)
        self.scraping_config = config_manager.get_scraping_config()
        
        self.logger.info("üîç Inicializando Web Searcher")
        
        # Configura√ß√µes de busca
        self.timeout = self.scraping_config['timeout']
        self.max_retries = self.scraping_config['max_retries']
        self.retry_delay = self.scraping_config['retry_delay']
        
        # Headers para requisi√ß√µes
        self.headers = {
            "User-Agent": self.scraping_config['user_agent'],
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # Sites irrelevantes para filtrar
        self.irrelevant_domains = {
            'facebook.com', 'instagram.com', 'twitter.com', 'x.com', 'linkedin.com',
            'youtube.com', 'tiktok.com', 'pinterest.com', 'reddit.com',
            'google.com', 'bing.com', 'yahoo.com', 'duckduckgo.com',
            'webcache.googleusercontent.com', 'translate.google.com',
            'amazon.com', 'ebay.com', 'alibaba.com'
        }
        
        self.logger.debug(f"Configura√ß√µes: timeout={self.timeout}s, retries={self.max_retries}")
    
    def search_organization_website(self, org_name: str) -> Tuple[Optional[str], str]:
        """
        Busca website de uma organiza√ß√£o com valida√ß√£o inteligente
        1. Tenta Wikipedia primeiro com valida√ß√£o de relev√¢ncia
        2. Se Wikipedia n√£o for relevante, usa Bing como fallback
        3. Se Bing falhar, usa Wikipedia mesmo que irrelevante
        
        Args:
            org_name: Nome da organiza√ß√£o
            
        Returns:
            Tuple com (URL encontrada, m√©todo usado)
        """
        self.logger.info(f"üîç Buscando website para: {org_name}")
        
        # 1. Tentar Wikipedia primeiro
        try:
            wiki_url, wiki_title = self.search_wikipedia_with_validation(org_name)
            if wiki_url and self._is_wikipedia_result_relevant(wiki_title, org_name):
                self.logger.info(f"‚ú® Website encontrado via Wikipedia (relevante): {wiki_url}")
                return wiki_url, "wikipedia"
            elif wiki_url:
                self.logger.warning(f"‚ö†Ô∏è Wikipedia encontrada mas irrelevante: {wiki_title} para {org_name}")
                # Guardar para usar como fallback se necess√°rio
                fallback_wiki = (wiki_url, wiki_title)
        except Exception as e:
            self.logger.debug(f"Falha na busca Wikipedia: {str(e)}")
            fallback_wiki = None
        
        # 2. Fallback para Bing (√∫nica engine que funciona)
        try:
            url = self.search_bing_working(org_name)
            if url:
                self.logger.info(f"‚ú® Website encontrado via Bing: {url}")
                return url, "bing"
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Falha na busca Bing: {str(e)}")
        
        # 3. Se Bing falhar, usar Wikipedia mesmo que irrelevante (melhor que nada)
        if 'fallback_wiki' in locals() and fallback_wiki:
            wiki_url, wiki_title = fallback_wiki
            self.logger.warning(f"‚ö†Ô∏è Usando Wikipedia irrelevante como √∫ltimo recurso: {wiki_url}")
            return wiki_url, "wikipedia_fallback"
        
        self.logger.error(f"‚ùå Nenhum website encontrado para: {org_name}")
        return None, "failed"
    
    def search_wikipedia_with_validation(self, org_name: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Busca na Wikipedia e retorna URL + t√≠tulo para valida√ß√£o
        
        Args:
            org_name: Nome da organiza√ß√£o
            
        Returns:
            Tuple com (URL, t√≠tulo) ou (None, None)
        """
        self.logger.debug(f"üìö Buscando na Wikipedia: {org_name}")
        
        try:
            # Usar API da Wikipedia
            search_url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': org_name,
                'srlimit': 1  # Apenas primeiro resultado
            }
            
            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=self.timeout,
                verify=False
            )
            
            if response.status_code == 200:
                data = response.json()
                search_results = data.get('query', {}).get('search', [])
                
                if search_results:
                    first_result = search_results[0]
                    title = first_result.get('title', '')
                    page_url = f"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}"
                    
                    self.logger.debug(f"Wikipedia encontrada: {title} -> {page_url}")
                    return page_url, title
            
            return None, None
            
        except Exception as e:
            self.logger.debug(f"Erro na busca Wikipedia: {str(e)}")
            return None, None
    
    def _is_wikipedia_result_relevant(self, wiki_title: str, org_name: str) -> bool:
        """
        Valida se o resultado da Wikipedia √© relevante para a organiza√ß√£o
        Verifica se palavras-chave da organiza√ß√£o aparecem no t√≠tulo
        
        Args:
            wiki_title: T√≠tulo da p√°gina da Wikipedia
            org_name: Nome da organiza√ß√£o
            
        Returns:
            True se o resultado √© relevante
        """
        if not wiki_title or not org_name:
            return False
        
        # Normalizar strings
        title_lower = wiki_title.lower()
        org_lower = org_name.lower()
        
        # Extrair palavras significativas da organiza√ß√£o (> 2 caracteres)
        org_words = [word for word in org_lower.split() if len(word) > 2]
        
        # Remover palavras comuns que n√£o s√£o distintivas
        common_words = {
            'ltd', 'inc', 'corp', 'corporation', 'company', 'group', 'limited', 
            'co', 'llc', 'se', 'sa', 'ag', 'gmbh', 'bv', 'nv', 'spa', 'srl', 
            'the', 'and', 'of', 'for', 'in', 'on', 'at', 'to', 'by', 'with'
        }
        distinctive_words = [word for word in org_words if word not in common_words]
        
        # Se n√£o h√° palavras distintivas, usar todas as palavras > 2 chars
        words_to_check = distinctive_words if distinctive_words else org_words
        
        if not words_to_check:
            return False
        
        # Contar quantas palavras aparecem no t√≠tulo
        matches = 0
        for word in words_to_check:
            if word in title_lower:
                matches += 1
        
        # Calcular score de relev√¢ncia
        relevance_score = matches / len(words_to_check)
        
        # Log para debug
        self.logger.debug(f"Relev√¢ncia Wikipedia: '{wiki_title}' para '{org_name}'")
        self.logger.debug(f"  Palavras para verificar: {words_to_check}")
        self.logger.debug(f"  Matches: {matches}/{len(words_to_check)} = {relevance_score:.2f}")
        
        # Considerar relevante se pelo menos 50% das palavras coincidem
        # OU se √© uma correspond√™ncia exata de palavra √∫nica
        is_relevant = relevance_score >= 0.5 or (len(words_to_check) == 1 and matches == 1)
        
        if is_relevant:
            self.logger.debug(f"  ‚úÖ Relevante (score: {relevance_score:.2f})")
        else:
            self.logger.debug(f"  ‚ùå Irrelevante (score: {relevance_score:.2f})")
        
        return is_relevant
    
    def search_bing_working(self, org_name: str) -> Optional[str]:
        """
        Busca no Bing usando seletores que funcionam
        
        Args:
            org_name: Nome da organiza√ß√£o
            
        Returns:
            URL encontrada ou None
        """
        self.logger.debug(f"üÖ±Ô∏è Buscando no Bing: {org_name}")
        
        query = f'"{org_name}"'
        search_url = f"https://www.bing.com/search?q={requests.utils.quote(query)}&count=10"
        
        try:
            response = requests.get(
                search_url,
                headers=self.headers,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            
            if response.status_code != 200:
                self.logger.debug(f"Bing retornou status {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Usar seletores que funcionaram no debug
            working_selectors = [
                'li.b_algo h2 a',  # Seletor principal
                '.b_algo a',       # Alternativo
                'h2 a'             # Mais geral
            ]
            
            for selector in working_selectors:
                results = soup.select(selector)
                self.logger.debug(f"Seletor '{selector}': {len(results)} resultados")
                
                for result in results[:5]:  # Primeiros 5 resultados
                    href = result.get('href', '')
                    
                    if href and href.startswith('http') and self._is_valid_result(href, org_name):
                        self.logger.debug(f"URL v√°lida encontrada: {href}")
                        return href
            
            self.logger.debug("Nenhuma URL v√°lida encontrada no Bing")
            return None
            
        except Exception as e:
            self.logger.debug(f"Erro na busca Bing: {str(e)}")
            return None
    
    def _is_valid_result(self, url: str, org_name: str) -> bool:
        """
        Valida se um resultado de busca √© relevante
        
        Args:
            url: URL do resultado
            org_name: Nome da organiza√ß√£o
            
        Returns:
            True se √© um resultado v√°lido
        """
        try:
            # Parse da URL
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            full_url = url.lower()
            
            self.logger.debug(f"Validando URL: {url}")
            
            # Remover www.
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # Filtrar dom√≠nios irrelevantes
            if any(irrelevant in domain for irrelevant in self.irrelevant_domains):
                self.logger.debug(f"URL rejeitada - dom√≠nio irrelevante: {domain}")
                return False
            
            # Verificar se √© um dom√≠nio v√°lido
            if not domain or '.' not in domain:
                self.logger.debug(f"URL rejeitada - dom√≠nio inv√°lido: {domain}")
                return False
            
            # Verificar se n√£o √© um subdom√≠nio suspeito
            suspicious_subdomains = ['translate.', 'webcache.', 'cached.']
            if any(domain.startswith(sub) for sub in suspicious_subdomains):
                self.logger.debug(f"URL rejeitada - subdom√≠nio suspeito: {domain}")
                return False
            
            # Filtrar URLs que claramente n√£o s√£o sites oficiais
            bad_patterns = [
                '/search?', '/q=', '/query=', '/results?'
            ]
            
            if any(pattern in full_url for pattern in bad_patterns):
                self.logger.debug(f"URL rejeitada - padr√£o suspeito na URL: {full_url}")
                return False
            
            # Validar se a URL responde
            if not self.validate_website_url(url):
                self.logger.debug(f"URL rejeitada - n√£o responde: {url}")
                return False
            
            # Se chegou at√© aqui, √© uma URL v√°lida
            self.logger.debug(f"URL aceita: {url}")
            return True
            
        except Exception as e:
            self.logger.debug(f"Erro na valida√ß√£o de {url}: {str(e)}")
            return False
    
    def validate_website_url(self, url: str) -> bool:
        """
        Valida se uma URL √© acess√≠vel
        
        Args:
            url: URL para validar
            
        Returns:
            True se a URL √© v√°lida e acess√≠vel
        """
        try:
            # Fazer HEAD request para verificar se o site responde
            response = requests.head(
                url,
                headers=self.headers,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            
            # Considerar c√≥digos de sucesso
            return response.status_code in [200, 301, 302, 303, 307, 308]
            
        except Exception:
            # Se HEAD falhar, tentar GET r√°pido
            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    timeout=self.timeout // 2,  # Timeout menor para valida√ß√£o
                    verify=False,
                    allow_redirects=True,
                    stream=True  # N√£o baixar o conte√∫do completo
                )
                return response.status_code in [200, 301, 302, 303, 307, 308]
            except Exception:
                return False
    
    def search_with_retry(self, org_name: str, max_attempts: int = 3) -> Tuple[Optional[str], str]:
        """
        Busca com retry autom√°tico em caso de falha
        
        Args:
            org_name: Nome da organiza√ß√£o
            max_attempts: N√∫mero m√°ximo de tentativas
            
        Returns:
            Tuple com (URL encontrada, m√©todo usado)
        """
        last_error = None
        
        for attempt in range(max_attempts):
            try:
                result = self.search_organization_website(org_name)
                if result[0]:  # Se encontrou URL
                    return result
                
                # Se n√£o encontrou, aguardar antes da pr√≥xima tentativa
                if attempt < max_attempts - 1:
                    wait_time = self.retry_delay * (attempt + 1)
                    self.logger.debug(f"Tentativa {attempt + 1} falhou, aguardando {wait_time}s...")
                    time.sleep(wait_time)
                    
            except Exception as e:
                last_error = e
                self.logger.warning(f"Erro na tentativa {attempt + 1}: {str(e)}")
                
                if attempt < max_attempts - 1:
                    time.sleep(self.retry_delay * (attempt + 1))
        
        # Se chegou aqui, todas as tentativas falharam
        error_msg = f"failed_after_{max_attempts}_attempts"
        if last_error:
            error_msg += f": {str(last_error)}"
        
        return None, error_msg


def main():
    """Fun√ß√£o para testar o web searcher"""
    searcher = WebSearcher()
    
    # Testar com algumas organiza√ß√µes conhecidas
    test_orgs = [
        "Microsoft Corporation",
        "World Bank Group", 
        "United Nations",
        "Allianz SE",
        "Swiss Re"
    ]
    
    print(f"\nüß™ Testando Web Searcher com {len(test_orgs)} organiza√ß√µes:")
    
    for org in test_orgs:
        print(f"\nüîç Buscando: {org}")
        url, method = searcher.search_organization_website(org)
        
        if url:
            print(f"  ‚úÖ Encontrado via {method}: {url}")
        else:
            print(f"  ‚ùå N√£o encontrado ({method})")


if __name__ == "__main__":
    main()