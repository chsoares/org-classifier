#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Web Searcher - Sistema de busca de websites de organiza√ß√µes

Este m√≥dulo implementa busca inteligente com:
1. Wikipedia primeiro com valida√ß√£o de relev√¢ncia
2. Bing como fallback se Wikipedia n√£o for relevante
3. Valida√ß√£o de URLs encontradas
"""

import requests
import urllib3
from bs4 import BeautifulSoup
from urllib.parse import urlparse, urljoin
import re
import time
from typing import Optional, Tuple
import sys
from pathlib import Path

# Desabilitar avisos de SSL
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# Adicionar src ao path para imports
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger_config import setup_logger
from utils.config_manager import config_manager


class WebSearcher:
    """
    Sistema de busca de websites de organiza√ß√µes com Wikipedia + Bing
    """
    
    def __init__(self):
        self.logger, _ = setup_logger("web_searcher", log_to_file=True)
        self.scraping_config = config_manager.get_scraping_config()
        
        self.logger.info("üîç Inicializando Web Searcher")
        
        # Configura√ß√µes de busca
        self.timeout = self.scraping_config['timeout']
        self.max_retries = self.scraping_config['max_retries']
        self.retry_delay = self.scraping_config['retry_delay']
        
        # Headers para requisi√ß√µes
        self.headers = {
            "User-Agent": self.scraping_config['user_agent'],
            "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
            "Accept-Language": "en-US,en;q=0.5",
            "Accept-Encoding": "gzip, deflate",
            "DNT": "1",
            "Connection": "keep-alive",
            "Upgrade-Insecure-Requests": "1",
        }
        
        # Sites irrelevantes para filtrar
        self.irrelevant_domains = {
            # Redes sociais
            'facebook.com', 'instagram.com', 'twitter.com', 'x.com', 'linkedin.com',
            'youtube.com', 'tiktok.com', 'pinterest.com', 'reddit.com',
            
            # Motores de busca
            'google.com', 'bing.com', 'yahoo.com', 'duckduckgo.com',
            'webcache.googleusercontent.com', 'translate.google.com',
            'baidu.com', 'sogou.com', 'yandex.com', 'ask.com',
            
            # E-commerce
            'amazon.com', 'ebay.com', 'alibaba.com', 'mercadolivre.com.br',
            
            # F√≥runs e Q&A
            'stackoverflow.com', 'stackexchange.com', 'superuser.com',
            'serverfault.com', 'askubuntu.com', 'mathoverflow.net',
            'quora.com', 'answers.yahoo.com', 'answers.com',
            'reddit.com', 'discourse.org',
            
            # F√≥runs espec√≠ficos por dom√≠nio
            'forum.', 'forums.', 'community.', 'discuss.', 'discussion.',
            
            # Sites de not√≠cias/blogs gen√©ricos
            'medium.com', 'blogger.com', 'wordpress.com', 'tumblr.com',
            'substack.com', 'ghost.org',
            
            # Diret√≥rios e agregadores
            'yellowpages.com', 'whitepages.com', 'yelp.com', 'foursquare.com',
            'zoominfo.com', 'crunchbase.com', 'bloomberg.com',
            
            # Sites de tradu√ß√£o e cache
            'translate.google.com', 'translate.bing.com', 'deepl.com',
            'webcache.googleusercontent.com', 'archive.org', 'web.archive.org',
            
            # Plataformas de desenvolvimento
            'github.com', 'gitlab.com', 'bitbucket.org', 'sourceforge.net',
            
            # Sites de emprego
            'indeed.com', 'glassdoor.com', 'monster.com', 'careerbuilder.com',
            'jobs.com', 'workday.com'
        }
        
        self.logger.debug(f"Configura√ß√µes: timeout={self.timeout}s, retries={self.max_retries}")
    
    def search_organization_website(self, org_name: str) -> Tuple[Optional[str], str]:
        """
        Busca website de uma organiza√ß√£o com valida√ß√£o inteligente
        1. Tenta Wikipedia primeiro com valida√ß√£o de relev√¢ncia
        2. Se Wikipedia n√£o for relevante, usa Bing como fallback
        3. Se Bing falhar, usa Wikipedia mesmo que irrelevante
        
        Args:
            org_name: Nome da organiza√ß√£o
            
        Returns:
            Tuple com (URL encontrada, m√©todo usado)
        """
        self.logger.info(f"üîç Buscando website para: {org_name}")
        
        # 1. Tentar Wikipedia primeiro
        try:
            wiki_url, wiki_title = self.search_wikipedia_with_validation(org_name)
            if wiki_url and self._is_wikipedia_result_relevant(wiki_title, org_name):
                self.logger.info(f"‚ú® Website encontrado via Wikipedia (relevante): {wiki_url}")
                return wiki_url, "wikipedia"
            elif wiki_url:
                self.logger.warning(f"‚ö†Ô∏è Wikipedia encontrada mas irrelevante: {wiki_title} para {org_name}")
                # Guardar para usar como fallback se necess√°rio
                fallback_wiki = (wiki_url, wiki_title)
        except Exception as e:
            self.logger.debug(f"Falha na busca Wikipedia: {str(e)}")
            fallback_wiki = None
        
        # 2. Fallback para Bing (√∫nica engine que funciona)
        try:
            url = self.search_bing_working(org_name)
            if url:
                self.logger.info(f"‚ú® Website encontrado via Bing: {url}")
                return url, "bing"
        except Exception as e:
            self.logger.warning(f"‚ö†Ô∏è Falha na busca Bing: {str(e)}")
        
        # 3. Se Bing falhar, usar Wikipedia mesmo que irrelevante (melhor que nada)
        if 'fallback_wiki' in locals() and fallback_wiki:
            wiki_url, wiki_title = fallback_wiki
            self.logger.warning(f"‚ö†Ô∏è Usando Wikipedia irrelevante como √∫ltimo recurso: {wiki_url}")
            return wiki_url, "wikipedia_fallback"
        
        self.logger.error(f"‚ùå Nenhum website encontrado para: {org_name}")
        return None, "failed"
    
    def search_wikipedia_with_validation(self, org_name: str) -> Tuple[Optional[str], Optional[str]]:
        """
        Busca na Wikipedia e retorna URL + t√≠tulo para valida√ß√£o
        
        Args:
            org_name: Nome da organiza√ß√£o
            
        Returns:
            Tuple com (URL, t√≠tulo) ou (None, None)
        """
        self.logger.debug(f"üìö Buscando na Wikipedia: {org_name}")
        
        try:
            # Usar API da Wikipedia
            search_url = "https://en.wikipedia.org/w/api.php"
            params = {
                'action': 'query',
                'format': 'json',
                'list': 'search',
                'srsearch': org_name,
                'srlimit': 1  # Apenas primeiro resultado
            }
            
            response = requests.get(
                search_url,
                params=params,
                headers=self.headers,
                timeout=self.timeout,
                verify=False
            )
            
            if response.status_code == 200:
                data = response.json()
                search_results = data.get('query', {}).get('search', [])
                
                if search_results:
                    first_result = search_results[0]
                    title = first_result.get('title', '')
                    page_url = f"https://en.wikipedia.org/wiki/{title.replace(' ', '_')}"
                    
                    self.logger.debug(f"Wikipedia encontrada: {title} -> {page_url}")
                    return page_url, title
            
            return None, None
            
        except Exception as e:
            self.logger.debug(f"Erro na busca Wikipedia: {str(e)}")
            return None, None
    
    def _is_wikipedia_result_relevant(self, wiki_title: str, org_name: str) -> bool:
        """
        Valida se o resultado da Wikipedia √© relevante para a organiza√ß√£o
        Verifica se palavras-chave da organiza√ß√£o aparecem no t√≠tulo
        
        Args:
            wiki_title: T√≠tulo da p√°gina da Wikipedia
            org_name: Nome da organiza√ß√£o
            
        Returns:
            True se o resultado √© relevante
        """
        if not wiki_title or not org_name:
            return False
        
        # Normalizar strings
        title_lower = wiki_title.lower()
        org_lower = org_name.lower()
        
        # Extrair palavras significativas da organiza√ß√£o (> 2 caracteres)
        org_words = [word for word in org_lower.split() if len(word) > 2]
        
        # Remover palavras comuns que n√£o s√£o distintivas
        common_words = {
            'ltd', 'inc', 'corp', 'corporation', 'company', 'group', 'limited', 
            'co', 'llc', 'se', 'sa', 'ag', 'gmbh', 'bv', 'nv', 'spa', 'srl', 
            'the', 'and', 'of', 'for', 'in', 'on', 'at', 'to', 'by', 'with'
        }
        distinctive_words = [word for word in org_words if word not in common_words]
        
        # Se n√£o h√° palavras distintivas, usar todas as palavras > 2 chars
        words_to_check = distinctive_words if distinctive_words else org_words
        
        if not words_to_check:
            return False
        
        # Contar quantas palavras aparecem no t√≠tulo
        matches = 0
        for word in words_to_check:
            if word in title_lower:
                matches += 1
        
        # Calcular score de relev√¢ncia
        relevance_score = matches / len(words_to_check)
        
        # Log para debug
        self.logger.debug(f"Relev√¢ncia Wikipedia: '{wiki_title}' para '{org_name}'")
        self.logger.debug(f"  Palavras para verificar: {words_to_check}")
        self.logger.debug(f"  Matches: {matches}/{len(words_to_check)} = {relevance_score:.2f}")
        
        # Considerar relevante se pelo menos 50% das palavras coincidem
        # OU se √© uma correspond√™ncia exata de palavra √∫nica
        is_relevant = relevance_score >= 0.5 or (len(words_to_check) == 1 and matches == 1)
        
        if is_relevant:
            self.logger.debug(f"  ‚úÖ Relevante (score: {relevance_score:.2f})")
        else:
            self.logger.debug(f"  ‚ùå Irrelevante (score: {relevance_score:.2f})")
        
        return is_relevant
    
    def search_bing_working(self, org_name: str) -> Optional[str]:
        """
        Busca no Bing usando seletores que funcionam
        
        Args:
            org_name: Nome da organiza√ß√£o
            
        Returns:
            URL encontrada ou None
        """
        self.logger.debug(f"üÖ±Ô∏è Buscando no Bing: {org_name}")
        
        query = f'"{org_name}"'
        search_url = f"https://www.bing.com/search?q={requests.utils.quote(query)}&count=10"
        
        try:
            response = requests.get(
                search_url,
                headers=self.headers,
                timeout=self.timeout,
                verify=False,
                allow_redirects=True
            )
            
            if response.status_code != 200:
                self.logger.debug(f"Bing retornou status {response.status_code}")
                return None
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Usar seletores mais precisos baseado em testes reais
            working_selectors = [
                'li.b_algo h2 a',           # Seletor principal - mais preciso
                'h2 a',                     # Alternativo para t√≠tulos principais  
                '.b_algo h2 a'              # Backup sem li
            ]
            
            # Coletar todas as URLs candidatas primeiro
            candidate_urls = []
            
            for selector in working_selectors:
                results = soup.select(selector)
                self.logger.debug(f"Seletor '{selector}': {len(results)} resultados")
                
                for result in results[:5]:  # Primeiros 5 resultados
                    href = result.get('href', '')
                    if href and href.startswith('http'):
                        candidate_urls.append(href)
            
            # Remover duplicatas mantendo ordem
            seen = set()
            unique_urls = []
            for url in candidate_urls:
                if url not in seen:
                    seen.add(url)
                    unique_urls.append(url)
            
            # Ordenar por relev√¢ncia de dom√≠nio
            url_scores = []
            for url in unique_urls[:10]:  # Limitar a 10 para performance
                parsed = urlparse(url)
                domain = parsed.netloc.lower()
                if domain.startswith('www.'):
                    domain = domain[4:]
                
                relevance = self._calculate_domain_relevance(domain, org_name)
                
                # Bonus adicional para dom√≠nios que parecem oficiais
                if self._is_likely_official_domain(domain, org_name):
                    relevance *= 1.3
                    self.logger.debug(f"  üè¢ Bonus para dom√≠nio oficial: {domain}")
                
                url_scores.append((url, relevance, domain))
            
            # Ordenar por relev√¢ncia (maior primeiro), depois por simplicidade do dom√≠nio
            url_scores.sort(key=lambda x: (x[1], -len(x[2].split('.'))), reverse=True)
            
            # Testar URLs em ordem de relev√¢ncia
            for url, score, domain in url_scores:
                self.logger.debug(f"Testando URL (relev√¢ncia {score:.2f}): {url}")
                if self._is_valid_result(url, org_name):
                    self.logger.debug(f"URL v√°lida encontrada: {url}")
                    return url
            
            self.logger.debug("Nenhuma URL v√°lida encontrada no Bing")
            return None
            
        except Exception as e:
            self.logger.debug(f"Erro na busca Bing: {str(e)}")
            return None
    
    def _is_valid_result(self, url: str, org_name: str) -> bool:
        """
        Valida se um resultado de busca √© relevante
        
        Args:
            url: URL do resultado
            org_name: Nome da organiza√ß√£o
            
        Returns:
            True se √© um resultado v√°lido
        """
        try:
            # Parse da URL
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            full_url = url.lower()
            
            self.logger.debug(f"Validando URL: {url}")
            
            # Remover www.
            if domain.startswith('www.'):
                domain = domain[4:]
            
            # Filtrar dom√≠nios irrelevantes (mas permitir dom√≠nios principais de empresas conhecidas)
            main_corporate_domains = ['google.com', 'microsoft.com', 'apple.com', 'amazon.com']
            is_main_corporate = any(domain == corp_domain for corp_domain in main_corporate_domains)
            
            if not is_main_corporate and any(irrelevant in domain for irrelevant in self.irrelevant_domains):
                self.logger.debug(f"URL rejeitada - dom√≠nio irrelevante: {domain}")
                return False
            
            # Verificar se √© um dom√≠nio v√°lido
            if not domain or '.' not in domain:
                self.logger.debug(f"URL rejeitada - dom√≠nio inv√°lido: {domain}")
                return False
            
            # Verificar se n√£o √© um subdom√≠nio suspeito
            suspicious_subdomains = ['translate.', 'webcache.', 'cached.']
            if any(domain.startswith(sub) for sub in suspicious_subdomains):
                self.logger.debug(f"URL rejeitada - subdom√≠nio suspeito: {domain}")
                return False
            
            # Filtrar URLs que claramente n√£o s√£o sites oficiais
            bad_patterns = [
                # Padr√µes de busca
                '/search?', '/q=', '/query=', '/results?', '/find?',
                
                # Padr√µes de f√≥runs e Q&A
                '/questions/', '/question/', '/answers/', '/answer/',
                '/forum/', '/forums/', '/community/', '/discuss/',
                '/thread/', '/topic/', '/post/', '/posts/',
                '/ask/', '/help/', '/support/',
                
                # Padr√µes de conte√∫do gen√©rico
                '/blog/', '/news/', '/article/', '/articles/',
                '/review/', '/reviews/', '/rating/', '/ratings/',
                '/tag/', '/tags/', '/category/', '/categories/',
                
                # Padr√µes de sites espec√≠ficos
                'stackoverflow.com/', 'stackexchange.com/', 'quora.com/',
                'reddit.com/', 'medium.com/', 'answers.yahoo.com/',
                
                # Padr√µes de tradu√ß√£o e cache
                'translate.google.com/', 'webcache.googleusercontent.com/',
                'archive.org/', 'web.archive.org/',
                
                # Padr√µes de diret√≥rios
                'yellowpages.com/', 'whitepages.com/', 'yelp.com/',
                'zoominfo.com/', 'crunchbase.com/'
            ]
            
            if any(pattern in full_url for pattern in bad_patterns):
                self.logger.debug(f"URL rejeitada - padr√£o suspeito na URL: {full_url}")
                return False
            
            # Verifica√ß√£o espec√≠fica para sites de f√≥runs e Q&A
            forum_indicators = [
                'forum', 'community', 'discuss', 'questions', 'answers',
                'stackoverflow', 'stackexchange', 'quora', 'reddit'
            ]
            
            if any(indicator in domain or indicator in full_url for indicator in forum_indicators):
                self.logger.debug(f"URL rejeitada - site de f√≥rum/Q&A detectado: {full_url}")
                return False
            
            # NOVO: Validar relev√¢ncia do dom√≠nio com o nome da organiza√ß√£o
            relevance_score = self._calculate_domain_relevance(domain, org_name)
            self.logger.debug(f"Score de relev√¢ncia do dom√≠nio: {relevance_score:.2f}")
            
            # Se o dom√≠nio √© muito irrelevante, rejeitar mesmo que responda
            if relevance_score < 0.1:
                self.logger.debug(f"URL rejeitada - dom√≠nio irrelevante (score: {relevance_score:.2f})")
                return False
            
            # Para dom√≠nios com alta relev√¢ncia, ser menos restritivo na valida√ß√£o de conectividade
            if relevance_score >= 0.7:
                # Dom√≠nios muito relevantes: apenas verificar se n√£o s√£o claramente inv√°lidos
                self.logger.debug(f"URL com alta relev√¢ncia - valida√ß√£o relaxada: {url}")
                return True
            
            # Para dom√≠nios com relev√¢ncia m√©dia/baixa, validar conectividade
            if not self.validate_website_url(url):
                self.logger.debug(f"URL rejeitada - n√£o responde: {url}")
                return False
            
            # Se chegou at√© aqui, √© uma URL v√°lida
            self.logger.debug(f"URL aceita: {url} (relev√¢ncia: {relevance_score:.2f})")
            return True
            
        except Exception as e:
            self.logger.debug(f"Erro na valida√ß√£o de {url}: {str(e)}")
            return False
    
    def _calculate_domain_relevance(self, domain: str, org_name: str) -> float:
        """
        Calcula score de relev√¢ncia entre dom√≠nio e nome da organiza√ß√£o
        
        Args:
            domain: Dom√≠nio da URL (ex: coldiretti.it)
            org_name: Nome da organiza√ß√£o (ex: Coldiretti)
            
        Returns:
            Score de 0.0 a 1.0 (1.0 = muito relevante)
        """
        if not domain or not org_name:
            return 0.0
        
        domain_clean = domain.lower().replace('-', ' ').replace('_', ' ')
        org_clean = org_name.lower().replace('-', ' ').replace('_', ' ')
        
        # Extrair palavras significativas (> 2 caracteres)
        org_words = [word for word in org_clean.split() if len(word) > 2]
        
        # Remover palavras comuns
        common_words = {
            'ltd', 'inc', 'corp', 'corporation', 'company', 'group', 'limited', 
            'co', 'llc', 'se', 'sa', 'ag', 'gmbh', 'the', 'and', 'of', 'for'
        }
        distinctive_words = [word for word in org_words if word not in common_words]
        words_to_check = distinctive_words if distinctive_words else org_words
        
        if not words_to_check:
            return 0.0
        
        # Separar dom√≠nio principal de subdom√≠nios
        domain_parts = domain.split('.')
        main_domain_part = domain_parts[0] if domain_parts else domain
        
        # Contar matches no dom√≠nio completo
        matches = 0
        for word in words_to_check:
            if word in domain_clean:
                matches += 1
        
        # Calcular score base
        base_score = matches / len(words_to_check)
        
        # NOVO: Bonus significativo para dom√≠nios principais vs subdom√≠nios
        if len(words_to_check) == 1:
            main_word = words_to_check[0]
            
            # Caso ideal: palavra exata no dom√≠nio principal (ex: coldiretti.it)
            if main_word == main_domain_part:
                base_score = 1.0
                self.logger.debug(f"  üéØ Match perfeito no dom√≠nio principal: {main_word} = {main_domain_part}")
            
            # Caso bom: palavra no dom√≠nio principal mas n√£o exata (ex: coldiretti-news.it)
            elif main_word in main_domain_part:
                base_score = 0.9
                self.logger.debug(f"  ‚úÖ Match no dom√≠nio principal: {main_word} em {main_domain_part}")
            
            # Caso subdom√≠nio: palavra em subdom√≠nio (ex: polo.coldiretti.it)
            elif any(main_word in part for part in domain_parts[1:]):
                base_score = 0.7  # Menor score para subdom√≠nios
                self.logger.debug(f"  ‚ö†Ô∏è Match em subdom√≠nio: {main_word} em {domain}")
        
        # Bonus para dom√≠nios mais simples (menos subdom√≠nios)
        subdomain_count = len(domain_parts) - 2  # -2 para remover nome e TLD
        if subdomain_count == 0:
            base_score *= 1.2  # Bonus para dom√≠nio principal
        elif subdomain_count == 1:
            base_score *= 1.0  # Neutro para um subdom√≠nio
        else:
            base_score *= 0.8  # Penalidade para m√∫ltiplos subdom√≠nios
        
        # Penalty para dom√≠nios claramente irrelevantes
        irrelevant_indicators = [
            # F√≥runs e discuss√µes
            'forum', 'forums', 'community', 'discuss', 'discussion', 'board', 'boards',
            'qa', 'questions', 'answers', 'ask', 'help', 'support',
            
            # Conte√∫do gen√©rico
            'blog', 'news', 'article', 'tag', 'search', 'directory', 'wiki',
            'review', 'reviews', 'rating', 'ratings',
            
            # Sites espec√≠ficos problem√°ticos
            'stackoverflow', 'stackexchange', 'quora', 'reddit', 'medium',
            'baidu', 'sina', 'qq', 'sogou', 'yandex',
            'uol.com', 'globo.com', 'tuacasa', 'espaco-zen',
            
            # Padr√µes de subdom√≠nios suspeitos
            'translate', 'cache', 'webcache', 'archive', 'wayback',
            'jobs', 'careers', 'hiring', 'recruitment',
            'shop', 'store', 'buy', 'sell', 'marketplace',
            
            # Agregadores e diret√≥rios
            'yellowpages', 'whitepages', 'yelp', 'foursquare', 'zoominfo',
            'crunchbase', 'bloomberg', 'reuters', 'associated-press'
        ]
        
        for indicator in irrelevant_indicators:
            if indicator in domain_clean:
                base_score *= 0.1  # Penalidade severa
                self.logger.debug(f"  ‚ùå Penalidade por indicador irrelevante: {indicator}")
                break
        
        # Penalty para subdom√≠nios suspeitos
        suspicious_subdomains = [
            'www', 'blog', 'news', 'forum', 'forums', 'community', 'discuss',
            'shop', 'store', 'portal', 'polo', 'help', 'support', 'ask',
            'questions', 'answers', 'qa', 'wiki', 'docs', 'translate'
        ]
        if domain_parts and domain_parts[0] in suspicious_subdomains:
            base_score *= 0.8
            self.logger.debug(f"  ‚ö†Ô∏è Penalidade por subdom√≠nio suspeito: {domain_parts[0]}")
        
        return min(base_score, 1.0)
    
    def _is_likely_official_domain(self, domain: str, org_name: str) -> bool:
        """
        Verifica se um dom√≠nio parece ser o site oficial da organiza√ß√£o
        
        Args:
            domain: Dom√≠nio a verificar
            org_name: Nome da organiza√ß√£o
            
        Returns:
            True se parece ser dom√≠nio oficial
        """
        if not domain or not org_name:
            return False
        
        domain_parts = domain.split('.')
        if len(domain_parts) < 2:
            return False
        
        main_part = domain_parts[0]
        org_clean = org_name.lower().replace(' ', '').replace('-', '').replace('_', '')
        
        # Casos que indicam dom√≠nio oficial:
        # 1. Nome da organiza√ß√£o exato no dom√≠nio principal
        if org_clean in main_part or main_part in org_clean:
            # 2. TLD apropriado (.com, .org, .it, .de, etc.)
            tld = domain_parts[-1]
            official_tlds = ['com', 'org', 'net', 'gov', 'edu', 'it', 'de', 'fr', 'uk', 'co', 'io']
            if tld in official_tlds:
                # 3. N√£o √© subdom√≠nio suspeito
                if len(domain_parts) <= 3:  # dominio.com ou subdominio.dominio.com
                    return True
        
        return False
    
    def validate_website_url(self, url: str) -> bool:
        """
        Valida se uma URL √© acess√≠vel
        
        Args:
            url: URL para validar
            
        Returns:
            True se a URL √© v√°lida e acess√≠vel
        """
        try:
            # Fazer HEAD request para verificar se o site responde
            response = requests.head(
                url,
                headers=self.headers,
                timeout=5,  # Timeout mais curto para valida√ß√£o
                verify=False,
                allow_redirects=True
            )
            
            # Considerar c√≥digos de sucesso e alguns erros que podem ser tempor√°rios
            success_codes = [200, 301, 302, 303, 307, 308, 403, 405]  # 403/405 podem ser sites v√°lidos com restri√ß√µes
            return response.status_code in success_codes
            
        except Exception:
            # Se HEAD falhar, tentar GET r√°pido
            try:
                response = requests.get(
                    url,
                    headers=self.headers,
                    timeout=3,  # Timeout ainda menor para GET
                    verify=False,
                    allow_redirects=True,
                    stream=True  # N√£o baixar o conte√∫do completo
                )
                success_codes = [200, 301, 302, 303, 307, 308, 403, 405]
                return response.status_code in success_codes
            except Exception:
                # Se ambos falharem, assumir que pode ser um problema tempor√°rio
                # Para dom√≠nios que parecem leg√≠timos, ser mais tolerante
                parsed = urlparse(url)
                domain = parsed.netloc.lower()
                if domain.startswith('www.'):
                    domain = domain[4:]
                
                # Dom√≠nios conhecidos que podem ter problemas tempor√°rios
                known_domains = [
                    'microsoft.com', 'apple.com', 'google.com', 'amazon.com',
                    'facebook.com', 'twitter.com', 'linkedin.com', 'github.com',
                    'coldiretti.it', 'allianz.com', 'bmw.com', 'volkswagen.com'
                ]
                
                if any(known in domain for known in known_domains):
                    self.logger.debug(f"Dom√≠nio conhecido com problema tempor√°rio: {domain}")
                    return True
                
                return False
    
    def search_with_retry(self, org_name: str, max_attempts: int = 3) -> Tuple[Optional[str], str]:
        """
        Busca com retry autom√°tico em caso de falha
        
        Args:
            org_name: Nome da organiza√ß√£o
            max_attempts: N√∫mero m√°ximo de tentativas
            
        Returns:
            Tuple com (URL encontrada, m√©todo usado)
        """
        last_error = None
        
        for attempt in range(max_attempts):
            try:
                result = self.search_organization_website(org_name)
                if result[0]:  # Se encontrou URL
                    return result
                
                # Se n√£o encontrou, aguardar antes da pr√≥xima tentativa
                if attempt < max_attempts - 1:
                    wait_time = self.retry_delay * (attempt + 1)
                    self.logger.debug(f"Tentativa {attempt + 1} falhou, aguardando {wait_time}s...")
                    time.sleep(wait_time)
                    
            except Exception as e:
                last_error = e
                self.logger.warning(f"Erro na tentativa {attempt + 1}: {str(e)}")
                
                if attempt < max_attempts - 1:
                    time.sleep(self.retry_delay * (attempt + 1))
        
        # Se chegou aqui, todas as tentativas falharam
        error_msg = f"failed_after_{max_attempts}_attempts"
        if last_error:
            error_msg += f": {str(last_error)}"
        
        return None, error_msg


def main():
    """Fun√ß√£o para testar o web searcher"""
    searcher = WebSearcher()
    
    # Testar com algumas organiza√ß√µes conhecidas
    test_orgs = [
        "Microsoft Corporation",
        "World Bank Group", 
        "United Nations",
        "Allianz SE",
        "Swiss Re"
    ]
    
    print(f"\nüß™ Testando Web Searcher com {len(test_orgs)} organiza√ß√µes:")
    
    for org in test_orgs:
        print(f"\nüîç Buscando: {org}")
        url, method = searcher.search_organization_website(org)
        
        if url:
            print(f"  ‚úÖ Encontrado via {method}: {url}")
        else:
            print(f"  ‚ùå N√£o encontrado ({method})")


if __name__ == "__main__":
    main()